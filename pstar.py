# -*- coding: utf-8 -*-
"""pstar.ipynb

Automatically generated by coLaboratory.

Original file is located at
    undefined

# imports
"""

import collections
from collections import defaultdict
from collections import namedtuple

import functools
from functools import partial
import getpass
import hashlib
import json
from multiprocessing import Pool
import operator
import os
import pickle
import re
import time
import types

import numpy as np
import scipy as sp
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.colors
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from google3.pyglib import gfile

from colabtools import adhoc_import
from colabtools import sheets

with adhoc_import.Google3CitcClient('rl_predict', username='iansf', behavior='preferred'):
  from google3.experimental.users.iansf.logging.qj_global import qj
  from google3.experimental.users.iansf.image_utils import image_utils
  from google3.experimental.users.iansf.rl_predict import utils as U
  from google3.experimental.users.iansf.rl_predict import policy_gradient

  from google3.third_party.py import pandas as pd
  from google3.third_party.py.pandas.tools import plotting

  U = reload(U)
  
pd.set_option('display.max_rows', 1000)

figsize = (16, 12)
sns.set_context('paper', rc={'figure.figsize': figsize})
sns.set_style('whitegrid')
sns.palplot(sns.color_palette('Set1', 10))
sns.palplot(sns.color_palette('Set2', 10))
sns.palplot(sns.color_palette('Set3', 10))
sns.set_palette(sns.color_palette('Set1', 10))

"""# pstar

This is the currently canonical implementation of my python container class replacement library.  The fun stuff is in pist, which replaces list.

## pict
"""

class pict(dict):  # pylint: disable=invalid-name
  """Dict where everything is automatically a property."""

  def __init__(self, *a, **kw):
    dict.__init__(self, *a, **kw)
    self.__dict__ = self

  def __cmp__(self, other):
    return self is other

  __eq__ = __cmp__
  
  def __ne__(self, other):
    return not self == other

  def __getitem__(self, key):
    if isinstance(key, list):
      return pist([self[x] for x in key])
    else:
      return dict.__getitem__(self, key)
#       try:
#         return dict.__getitem__(self, key)
#       except KeyError:
#         try:
#           if hasattr(self, key):
#             return getattr(self, key)
#         except AttributeError:
#           raise KeyError('\'%s\' is neither a key nor an attribute in this \'%s\'' % (key, type(self)))


  def __setitem__(self, key, value):
    if isinstance(key, list) and not isinstance(value, types.StringTypes) and hasattr(value, '__len__') and len(value) == len(key):
      for k, v in zip(key, value):
        dict.__setitem__(self, k, v)
    else:
      dict.__setitem__(self, key, value)

  def update(self, *a, **kw):
    dict.update(self, *a, **kw)
    return self
  
  def copy(self):
    return pict(dict.copy(self))

"""## defaultpict"""

class defaultpict(defaultdict):  # pylint: disable=invalid-name
  """Default dict where everything is automatically a property."""

  def __init__(self, *a, **kw):
    defaultdict.__init__(self, *a, **kw)

  def __getattr__(self, name):
    if name.startswith('_'):
      return defaultdict.__getattribute__(self, name)
#       raise AttributeError('\'%s\' object has no attribute \'%s\'' % (type(self), name))
    if name == '*':
      return pist([self[k] for k in self])
    return self[name]

  def __setattr__(self, name, value):
    self[name] = value

  def __cmp__(self, other):
    return self is other

  __eq__ = __cmp__

  def __ne__(self, other):
    return not self == other

  def __str__(self):
    delim = ', ' if len(self) < 8 else ',\n '
    s = delim.join('%s: %s' % (repr(k), repr(self[k])) for k in sorted(self))
    return '{' + s + '}'

  __repr__ = __str__

  def __getitem__(self, key):
    if isinstance(key, list):
      return pist([self[x] for x in key])
    else:
      return defaultdict.__getitem__(self, key)
#       try:
#         return defaultdict.__getitem__(self, key)
#       except KeyError:
#         try:
#           if hasattr(self, key):
#             return getattr(self, key)
#         except AttributeError:
#           raise KeyError('\'%s\' is neither a key nor an attribute in this \'%s\'' % (key, type(self)))

  def __setitem__(self, key, value):
    if isinstance(key, list) and not isinstance(value, types.StringTypes) and hasattr(value, '__len__') and len(value) == len(key):
      for k, v in zip(key, value):
        defaultdict.__setitem__(self, k, v)
    else:
      defaultdict.__setitem__(self, key, value)

  def update(self, *a, **kw):
    defaultdict.update(self, *a, **kw)
    return self
  
  def copy(self):
    return defaultpict(defaultdict.copy(self))

"""## pet"""

class pet(set):
  pass

"""## pist"""

dbg = 0
class pist(list):  # pylint: disable=invalid-name
  """List where everything is automatically a property that is applied to its elements.  Guaranteed to surprise, if not delight."""

  def __init__(self, *args, **kwargs):
    depth = qj(kwargs.pop('depth', 1), b=0)
    self.__root__ = qj(kwargs.pop('root', self), b=0)
    if depth == 1:
      list.__init__(self, *args, **kwargs)
    else:
      # Don't pass root through when making nested pists, because that doesn't make any sense.
      pist.__init__(self, [pist(*args, depth=depth - 1, **kwargs)])

  def __getattribute__(self, name):
    if name == '__root__':
      return list.__getattribute__(self, name)
    if name.startswith('___'):
      # Let people call reserved members of elements by using, e.g., ___len__().
      name = name[1:]
    elif name.startswith('__') and name.endswith('__'):
      raise qj(AttributeError('\'%s\'  objects cannot call reserved members of their elements: \'%s\'' % (type(self), name)), l=lambda _: self, b=0*dbg)
    qj(self, name, b=dbg)
    try:
      return qj(self.__getattr__(name), name, b=dbg)
    except AttributeError:
      qj(self, 'caught AttributeError for %s' % name, b=dbg)
      pass
    if not name.startswith('__') and name.endswith('_'):
      # Allows calling one level deeper by adding '_' to the end of a property name.  This is recursive, so '__' on the end goes two levels deep, etc.
      name = name[:-1]
    try:
      qj(name, 'trying', b=0)
      return qj(pist([(qj(hasattr(*qj((x, name), 'calling hasattr', l=lambda y: (self, y[0], type(y[0])), b=dbg)), 'hasattr %s' % name, l=lambda _: (self, x, type(x)), b=dbg)
                       and getattr(x, name))
                      or x[name] for x in self], root=self.__root__), name, b=dbg)
    except Exception as e:
      raise qj(AttributeError('\'%s\' object has no attribute \'%s\' (%s)' % (type(self), name, str(e))), l=lambda _: self, b=dbg)
  
  def __getattr__(self, name):
    qj(self, name, b=dbg * (not name.startswith('__')))
    attr = None
    if attr is None:
      attr = list.__getattribute__(self, name)
    def not_none_or_self(x):
      if x is None:
        return self
      return x
    return qj(lambda *a, **k: not_none_or_self(attr(*a, **k)), name, l=lambda _: self, b=dbg * (not name.startswith('__')))

  def __getitem__(self, key):
    try:
      if (isinstance(key, list)
          and pist(key).all(isinstance, int)):
        return qj(pist([self[k] for k in key]), 'return self[%s]' % str(key), b=0)  # Don't pass root -- we are uprooting
      else:
        return qj(list.__getitem__(self, key), 'return list[%s]' % str(key), b=0)
    except TypeError as first_exception:
      try:
        if isinstance(key, list):
          return qj(pist([self[i][k] for i, k in enumerate(key)]), 'return self[i][%s]' % 'str(key)', b=0)  # Don't pass root -- we are uprooting
        if isinstance(key, tuple):
          try:
            return pist([x[key] for x in self], root=self.__root__)
          except Exception:
            return qj(pist([tuple(x[k] for k in key) for x in self], root=self.__root__), 'return self[%s]' % 'str(key)', b=0)
        return qj(pist([x[key] for x in self], root=self.__root__), 'return elements[%s]' % 'str(key)', b=0)
      except Exception as second_exception:
        raise TypeError('Failed to apply index to self or elements.\nself exception: %s\nelements exception: %s' % (str(first_exception), str(second_exception)))

  def __getslice__(self, i, j):
    if self is self.__root__:
      return pist(list.__getslice__(self, i, j))
    return pist(list.__getslice__(self, i, j), root=pist(list.__getslice__(self.__root__, i, j)))
  
  def __setattr__(self, name, value):
    if name == '__root__':
      list.__setattr__(self, name, value)
    elif not isinstance(value, types.StringTypes) and hasattr(value, '__len__') and len(value) == len(self):
      for x, v in zip(self, value):
        x.__setattr__(name, v)
    else:
      for x in self:
        x.__setattr__(name, value)
  
  def __setitem__(self, key, value):
    list.__setitem__(self, key, value)
    return self
  
  def __setslice__(self, i, j, sequence):
    list.__setslice__(self, i, j, sequence)
    return self
  
  def __delattr__(self, name):
    for x in self:
      x.__delattr__(name)
    return self
  
  def __delitem__(self, key):
    list.__delitem__(self, key)
    return self
  
  def __delslice__(self, i, j):
    list.__delslice__(self, i, j)
    return self
  
  __hash__ = None


  def __call__(self, *args, **kwargs):
    return qj(pist([x(*args, **kwargs) for x in self], root=self.__root__), '__call__', b=dbg)
  
  def __contains__(self, other):
    return list.__contains__(self, other)

  
  def __and__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        try:
          return pist([x & o for x, o in zip(self, other)])
        except Exception:
          pass
      self_flat = self.ungroup(-1)
      ids = set([id(x) for x in self_flat]) & set([id(x) for x in other.ungroup(-1)])
      return pist([x for x in self_flat if id(x) in ids])  # Don't pass root -- we are uprooting
    else:
      return pist([(x & other) for x in self], root=self.__root__)

  def __or__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        try:
          return pist([x | o for x, o in zip(self, other)])
        except Exception:
          pass
      self_flat = self.ungroup(-1)
      other_flat = other.ungroup(-1)
      ids = set([id(x) for x in self_flat]) | set([id(x) for x in other_flat])
      return pist(
          [ids.remove(id(x)) or x for x in self_flat if id(x) in ids] +
          [ids.remove(id(x)) or x for x in other_flat if id(x) in ids]
      )  # Don't pass root -- we are uprooting
    else:
      return pist([(x | other) for x in self], root=self.__root__)
  
  def __xor__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        try:
          return pist([x ^ o for x, o in zip(self, other)])
        except Exception:
          pass
      self_flat = self.ungroup(-1)
      other_flat = other.ungroup(-1)
      ids = set([id(x) for x in self_flat]) ^ set([id(x) for x in other_flat])
      return pist(
          [ids.remove(id(x)) or x for x in self_flat if id(x) in ids] +
          [ids.remove(id(x)) or x for x in other_flat if id(x) in ids]
      )  # Don't pass root -- we are uprooting
    else:
      return pist([(x ^ other) for x in self], root=self.__root__)
    

  def __add__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x + o for x, o in zip(self, other)], root=self.__root__)
    return pist([x + other for x in self], root=self.__root__)

  def __radd__(self, other):
    return pist([other + x for x in self], root=self.__root__)

  def __iadd__(self, other):
    if isinstance(other, pist):
      return pist(list.__iadd__(self, other))
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x += other
    return new_pist


  def __sub__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x - o for x, o in zip(self, other)], root=self.__root__)
    return pist([x - other for x in self], root=self.__root__)

  def __rsub__(self, other):
    return pist([other - x for x in self], root=self.__root__)

  def __isub__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x -= other
    return new_pist


  def __mul__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x * o for x, o in zip(self, other)], root=self.__root__)
    return pist([x * other for x in self], root=self.__root__)
  
  def __rmul__(self, other):
    return pist([other * x for x in self], root=self.__root__)

  def __imul__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x *= other
    return new_pist


  def __div__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x / o for x, o in zip(self, other)], root=self.__root__)
    return pist([x / other for x in self], root=self.__root__)

  def __rdiv__(self, other):
    return pist([other / x for x in self], root=self.__root__)

  def __idiv__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x /= other
    return new_pist


  def __mod__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x % o for x, o in zip(self, other)], root=self.__root__)
    return pist([x % other for x in self], root=self.__root__)
  
  def __rmod__(self, other):
    return pist([other % x for x in self], root=self.__root__)

  def __imod__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x %= other
    return new_pist


  def __floordiv__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x // o for x, o in zip(self, other)], root=self.__root__)
    return pist([x // other for x in self], root=self.__root__)

  def __divmod__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([divmod(x, o) for x, o in zip(self, other)], root=self.__root__)
    return pist([divmod(x, other) for x in self], root=self.__root__)

  def __pow__(self, *args):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([pow(x, *args) for x, o in zip(self, other)], root=self.__root__)
    return pist([pow(x, *args) for x in self], root=self.__root__)

  def __lshift__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x << o for x, o in zip(self, other)], root=self.__root__)
    return pist([x << other for x in self], root=self.__root__)

  def __rshift__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x >> o for x, o in zip(self, other)], root=self.__root__)
    return pist([x >> other for x in self], root=self.__root__)

  def __truediv__(self, other):
    if isinstance(other, pist):
      if len(self) == len(other):
        return pist([x / o for x, o in zip(self, other)], root=self.__root__)
    return pist([x / other for x in self], root=self.__root__)


  def __rtruediv__(self, other):
    return pist([other / x for x in self], root=self.__root__)
  
  def __rfloordiv__(self, other):
    return pist([other // x for x in self], root=self.__root__)
  
  def __rdivmod__(self, other):
    return pist([divmod(other, x) for x in self], root=self.__root__)
  
  def __rpow__(self, other):
    return pist([other ** x for x in self], root=self.__root__)
  
  def __rlshift__(self, other):
    return pist([other << x for x in self], root=self.__root__)
  
  def __rrshift__(self, other):
    return pist([other >> x for x in self], root=self.__root__)
  
  def __rand__(self, other):
    return pist([other & x for x in self], root=self.__root__)
  
  def __rxor__(self, other):
    return pist([other ^ x for x in self], root=self.__root__)
  
  def __ror__(self, other):
    return pist([other | x for x in self], root=self.__root__)
  

  def __itruediv__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x /= other
    return new_pist

  def __ifloordiv__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x //= other
    return new_pist

  def __imod__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x %= other
    return new_pist

  def __ipow__(self, *args):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x = pow(x, *args)
    return new_pist

  def __ilshift__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x <<= other
    return new_pist

  def __irshift__(self, other):
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x >>= other
    return new_pist

  def __iand__(self, other):
    qj(d=1)
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x &= other
    return new_pist

  def __ixor__(self, other):
    qj(d=1)
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x ^= other
    return new_pist

  def __ior__(self, other):
    qj(d=1)
    new_pist = pist(self, root=self.__root__)
    for x in new_pist:
      x |= other
    return new_pist


  def __neg__(self):
    return pist([-x for x in self], root=self.__root__)

  def __pos__(self):
    return pist([+x for x in self], root=self.__root__)

  def __abs__(self):
    return pist([abs(x) for x in self], root=self.__root__)

  def __invert__(self):
    return pist([~x for x in self], root=self.__root__)

  def __complex__(self):
    return pist([complex(x) for x in self], root=self.__root__)

  def __int__(self):
    return pist([int(x) for x in self], root=self.__root__)

  def __long__(self):
    return pist([long(x) for x in self], root=self.__root__)

  def __float__(self):
    return pist([float(x) for x in self], root=self.__root__)

  def __oct__(self):
    return pist([oct(x) for x in self], root=self.__root__)

  def __hex__(self):
    return pist([hex(x) for x in self], root=self.__root__)


  # Nope.  Crashes when trying to index by pists of lists of ints.
#   def __index__(self):
#     qj(d=1)
#     return pist([x.__index__() for x in self], root=self.__root__)


  def __enter__(self):
    qj(d=1)
    return pist([x.__enter__() for x in self], root=self.__root__)
    
  def __exit__(self, exc_type, exc_value, traceback):
    qj(d=1)
    return pist([x.__exit__(exc_type, exc_value, traceback) for x in self], root=self.__root__)

  
  def root(self):
    return self.__root__

  def uproot(self):
    self.__root__ = self
    return self

#   def and(self, root=True):
#     # Interesting idea, but definitely doesn't work like this
#     return pist([x.__root__.append(x) if isinstance(x, pist) else pist([x], root=self) for x in self], root=self.__root__)

  def all(self, func, *args, **kwargs):
    for x in self:
      if not func(x, *args, **kwargs):
        return pist()
    return self

  def any(self, func, *args, **kwargs):
    for x in self:
      if func(x, *args, **kwargs):
        return self
    return pist()

  def apply(self, func, *args, **kwargs):
    pepth = kwargs.pop('pepth', 0)
    args = [_ensure_len(len(self), a) for a in args]
    kwargs = {
        k: _ensure_len(len(self), v) for k, v in kwargs.items()
    }
    qj((self, func, args, kwargs), 'apply called', b=0)
    if pepth != 0:
      if pepth < 0:
        try:
          return pist([x.apply(func, pepth=pepth - 1, *[a[i] for a in args], **{k: v[i] for k, v in kwargs.items()}) for i, x in enumerate(self)], root=self.__root__)
        except Exception as e:
          # qj((self, func, e), 'caught exception applying function, ignoring.', b=0)
          pass
      else:
        return pist([x.apply(func, pepth=pepth - 1, *[a[i] for a in args], **{k: v[i] for k, v in kwargs.items()}) for i, x in enumerate(self)], root=self.__root__)
    if isinstance(func, str):
      func = self.__getattribute__(func)
      if hasattr(func, '__len__') and len(func) == len(self):
        return pist([func[i](*[a[i] for a in args], **{k: v[i] for k, v in kwargs.items()}) for i, x in enumerate(self)], root=self.__root__)
      else:
        # We should be calling a single function of a pist object.  If that's not the case, something odd is happening, and the crash is appropriate.
        return func(*[a[0] for a in args], **{k: v[0] for k, v in kwargs.items()})
    return pist([func(x, *[a[i] for a in args], **{k: v[i] for k, v in kwargs.items()}) for i, x in enumerate(self)], root=self.__root__)

  def groupby(self):
    try:
      return pist([x.groupby() for x in self])
    except Exception:
      groups = collections.OrderedDict()
      for i, x in enumerate(self):
        if x not in groups:
          groups[x] = pist()
        groups[x].append(self.__root__[i])
      return pist(groups.values())
  
  def join(self):
    return pist([self])
  
  def nonempty(self, r=1):
    if r > 1 or r < 0:
      try:
        new_pist = pist([x.nonempty(r=r - 1) for x in self if len(x)])
      except Exception:
        new_pist = self
    else:
      new_pist = self
    return pist([x for x in new_pist if len(x)])

  def np(self, *args, **kwargs):
    return pist([np.array(x, *args, **kwargs) for x in self], root=self.__root__)
  
  def pd(self, *args, **kwargs):
    return pd.DataFrame.from_records(list(self), *args, **kwargs)
  
  def pet(self):
    return pist([pet(x) for x in self], root=self.__root__)

  def pfill(self, v=0, s=None):
    s = pict(v=v, s=lambda: s.update(v=s.v + 1).v) if s is None else s
    try:
      return pist([x.pfill(s=s) for i, x in enumerate(self)], root=self.__root__)
    except Exception:
      return pist([s.s() for _ in range(len(self))], root=self.__root__)

  def lfill(self, v=0, s=None):
    s = pict(v=v, s=lambda: s.update(v=s.v + 1).v) if s is None else s
    try:
      return [x.lfill(s=s) for i, x in enumerate(self)]
    except Exception:
      return [s.s() for _ in range(len(self))]

  def pleft(self):
    return -self.pfill() + self.plen(-1).ungroup(-1)[0]

  def plen(self, r=1):
    if r > 1 or r < 0:
      try:
        return pist([sum(x.plen(r - 1) for x in self)], root=self.__root__)
      except Exception:
        pass
    return pist([len(self)], root=self.__root__)
  
  def rlen(self, r=1):
    if r > 1 or r < 0:
      try:
        return pist([x.plen(r - 1) for x in self], root=self.__root__)
      except Exception:
        pass
    return pist([len(self)], root=self.__root__)
  
  def pshape(self):
    try:
      return pist([x.pshape() for x in self], root=self.__root__)
    except Exception:
      return pist([len(self)], root=self.__root__)

  def preduce_eq(self):
    vals = pet()
    new_items = []
    new_roots = []
    not_root = (not self is self.__root__)
    for i, x in enumerate(self):
      if x in vals:
        continue
      vals.add(x)
      new_items.append(x)
      if not_root:
        new_roots.append(self.__root__[i])
    if not_root:
      return pist(new_items, root=pist(new_roots))
    return pist(new_items)
    
  def pstr(self):
    try:
      return pist([x.pstr() for x in self], root=self.__root__)
    except Exception:
      return pist([str(x) for x in self], root=self.__root__)

  def qj(self, *args, **kwargs):
    return qj(self, *args, **kwargs)
  
  def remix(self, *args, **kwargs):
    kwargs = {
        k: _ensure_len(len(self), v) for k, v in kwargs.items()
    }
    new_items = []
    for i, x in enumerate(self):
      y = pict(
          **{
              a: (hasattr(x, a) and getattr(x, a)) or x[a]
              for a in args
          }
      )
      y.update({k: v[i] for k, v in kwargs.items()})
      new_items.append(y)
    return pist(new_items)
  
  def sortby(self, key=None, reverse=False):
    key = key or (lambda x: x)
    sorted_inds = [i for i, _ in sorted(enumerate(self), key=lambda x: key(x[1]), reverse=reverse)]
    self.__root__[:] = self.__root__[sorted_inds]
    if self is not self.__root__:
      self[:] = self[sorted_inds]
    return self

  def ungroup(self, r=1):
    new_items = []
    for g in self:
      if not isinstance(g, list):
        if r < 0:
          return self
        raise ValueError('Called ungroup on a pist that has non-group children')
      for x in g:
        new_items.append(x)
    if r > 1 or r < 0 and len(new_items):
      return pist(new_items).ungroup(r - 1)
    return pist(new_items)

#   def ungroup(self, r=1):
#     if r != 0:
#       try:
#         return pist([x.ungroup(r - 1) for x in self.__root__])
#       except Exception:
#         pass
    
#     new_items = []
#     for g in self.__root__:
#       if not isinstance(g, list):
#         if r < 0:
#           return self
#         raise ValueError('Called ungroup on a pist that has non-group children')
#       for x in g:
#         new_items.append(x)
#     if r > 1 or r < 0:
#       return pist(new_items).ungroup(r - 1)
#     return pist(new_items)

  def values_like(self, value=0):
    values = _ensure_len(len(self), value)
    try:
      return pist([x.values_like(v) for x, v in zip(self, values)], root=self.__root__)
    except Exception:
      pass
    return pist([v for v in values], root=self.__root__)


  def __cmp__(self, other, return_inds=False):
    if self is other:
      if return_inds:
        return qj(self.apply('lfill', -1, pepth=-1), 'lfill(-1)', b=0)
      else:
        return self
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__cmp__(o, return_inds=True)
          inds.append(child_inds)
        elif x == o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__cmp__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__cmp__(o, return_inds=True), operator.__or__)
      qj((inds, self, other), 'inds self.cmp(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__cmp__(other, return_inds=True)
          inds.append(child_inds)
        elif x == other:
          inds.append(i)
    
    qj(inds, 'inds cmp self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds cmp self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return cmp self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

  __eq__ = __cmp__
  
  def __ne__(self, other, return_inds=False):
    if self is other:
      if return_inds:
        return []
      return pist()
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__ne__(o, return_inds=True)
          inds.append(child_inds)
        elif x != o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__ne__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__ne__(o, return_inds=True), operator.__and__)
      qj((inds, self, other), 'inds self.ne(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__ne__(other, return_inds=True)
          inds.append(child_inds)
        elif x != other:
          inds.append(i)
    
    qj(inds, 'inds ne self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds ne self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return ne self: %s other: %s' % ('str(self)', 'str(other)'), b=0)


  def __gt__(self, other, return_inds=False):
    if self is other:
      return pist()
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__gt__(o, return_inds=True)
          inds.append(child_inds)
        elif x > o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__gt__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__gt__(o, return_inds=True), operator.__and__)
      qj((inds, self, other), 'inds self.gt(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__gt__(other, return_inds=True)
          inds.append(child_inds)
        elif x > other:
          inds.append(i)
    
    qj(inds, 'inds gt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds gt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return gt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
  
  
  def __ge__(self, other, return_inds=False):
    if self is other:
      if return_inds:
        return qj(self.apply('lfill', -1, pepth=-1), 'lfill(-1)', b=0)
      else:
        return self
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__ge__(o, return_inds=True)
          inds.append(child_inds)
        elif x >= o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__ge__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__ge__(o, return_inds=True), operator.__and__)
      qj((inds, self, other), 'inds self.ge(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__ge__(other, return_inds=True)
          inds.append(child_inds)
        elif x >= other:
          inds.append(i)
    
    qj(inds, 'inds ge self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds ge self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return ge self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

  
  def __lt__(self, other, return_inds=False):
    if self is other:
      return pist()
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__lt__(o, return_inds=True)
          inds.append(child_inds)
        elif x < o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__lt__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__lt__(o, return_inds=True), operator.__and__)
      qj((inds, self, other), 'inds self.lt(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__lt__(other, return_inds=True)
          inds.append(child_inds)
        elif x < other:
          inds.append(i)
    
    qj(inds, 'inds lt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds lt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return lt self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
  
  
  def __le__(self, other, return_inds=False):
    if self is other:
      if return_inds:
        return qj(self.apply('lfill', -1, pepth=-1), 'lfill(-1)', b=0)
      else:
        return self
    qj((self, other), b=0)
    inds = []
    if isinstance(other, list) and len(self) == len(other):
      for i, (x, o) in enumerate(zip(self, other)):
        if isinstance(x, pist):
          child_inds = x.__le__(o, return_inds=True)
          inds.append(child_inds)
        elif x <= o:
          inds.append(i)
    elif isinstance(other, list) and len(other) > 0:
      inds = self.__le__(other[0], return_inds=True)
      for o in other[1:]:
        inds = _merge_indices(inds, self.__le__(o, return_inds=True), operator.__and__)
      qj((inds, self, other), 'inds self.le(other)', b=0)
    else:
      for i, x in enumerate(self):
        if isinstance(x, pist):
          child_inds = x.__le__(other, return_inds=True)
          inds.append(child_inds)
        elif x <= other:
          inds.append(i)
    
    qj(inds, 'inds le self: %s other: %s' % ('str(self)', 'str(other)'), b=0)
    if return_inds:
      return qj(inds, 'return inds le self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

    return qj(self.__root__[inds], 'return le self: %s other: %s' % ('str(self)', 'str(other)'), b=0)

def _ensure_len(length, x):
  if not isinstance(x, str) and not isinstance(x, tuple) and hasattr(x, '__len__') and len(x) == length:
    return x
  return [x for _ in range(length)]


def _merge_indices(left, right, op):
  qj((left, right), 'l,r', b=0)
  try:
    left_empty_or_ints = len(left) == 0 or pist(left).all(isinstance, int)
    right_empty_or_ints = len(right) == 0 or pist(right).all(isinstance, int)
    if left_empty_or_ints and right_empty_or_ints:
      sl = set(left)
      sr = set(right)
      return qj(sorted(list(op(sl, sr))), 'ret left op right', b=0)
  except Exception:
    pass
  try:
    return qj([_merge_indices(left[i], right[i], op) for i in range(max(len(left), len(right)))], 'ret merge(l[i], r[i])', b=0)
  except Exception:
    pass
  if isinstance(left, list) and isinstance(right, list):
    return qj(left.extend(right) or left, 'ret left.extend(right)', b=0)
  return qj([left, right], 'ret [l, r]', b=1, d=1)

"""### pist Tests"""

if pist().qj('empty'):
  qj('BAD!')
else:
  qj('GOOD!')

try:
  pist().ungroup(-1)
  qj('GOOD!')
except Exception:
  qj('BAD!')

deepist = pist([x for x in range(4)], depth=3)
deepist.qj('deepist')
_ = deepist.append__(4).qj('deepist')

foos = pist([defaultpict(lambda: defaultpict(pist)) for _ in range(3)])

foos.qj('foos')

foos.foo.bar.append_(3)
foos.append(4)
_ = foos.qj('foos')

foos = pist([pict(foo=i, bar=i % 2) for i in range(3)])
(foos.bar == 0).update(dict(baz=3))
_ = foos.qj('foos')

foos = pist([pict(foo=i, bar=i % 2) for i in range(3)])
(foos.bar == 0).baz = 3
_ = foos.qj('foos')

foos = pist([pict(foo=i, bar=i % 2) for i in range(3)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6

foos_by_bar = (pist([(foos.bar == bar) for bar in set(foos.bar)])).qj('SLOW! N^2! BAD!')
baz = foos_by_bar.baz.np_().qj('np').sum().qj('baz sum')
max_baz = baz[baz.np().argmax().apply(np.expand_dims, -1).tolist().qj('baz.argmax')].qj('baz[argmax]')
(baz == max_baz).qj('baz == max_baz').foo.qj('final foos').root().bin = 13
_ = foos_by_bar.qj('foos_by_bar')

foos = pist([pict(foo=i, bar=i % 2) for i in range(3)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6

foos_by_bar = foos.bar.groupby().qj('SPEEDY! GOOD!')
baz = foos_by_bar.baz.np_().qj('np').sum().qj('baz sum')
max_baz = baz[baz.np().argmax().apply(np.expand_dims, -1).tolist().qj('baz.argmax')].qj('baz[argmax]')
(baz == max_baz).qj('baz == max_baz').foo.qj('final foos').root().bin = 13

((foos.bar == 0) & (foos.baz == 3)).qj('and filtered foos')
_ = foos_by_bar.qj('foos_by_bar')

foos = pist([pict(foo=i, bar=i % 3) for i in range(5)])

foos_by_bar = foos.bar.groupby()

(foos_by_bar.bar == 0).qj('by_bar == 0')
(foos_by_bar.bar == 1).qj('by_bar == 1')
(foos_by_bar.bar == 2).qj('by_bar == 2')

foos_by_bar.qj('by_bar')
_ = foos_by_bar.join().qj('joined')

foos = pist([pict(foo=i, bar=i % 3) for i in range(5)])

foos_by_bar = foos.bar.groupby().qj('by_bar')

(foos_by_bar.bar > 0).qj('by_bar > 0')
(foos_by_bar.bar != 1).qj('by_bar != 1')
(foos_by_bar.bar < 2).qj('by_bar < 2')
(foos_by_bar.bar <= 1).qj('by_bar <= 1')
(foos_by_bar.bar >= 1).qj('by_bar >= 1')

None

foos = pist([pict(foo=i, bar=i % 3) for i in range(5)])
foos_by_bar = foos.bar.groupby().qj('by_bar')

((foos_by_bar.bar == 0) & (foos_by_bar.bar == 1)).qj('by_bar == 0 & by_bar == 1')
((foos_by_bar.bar == 0) & (foos_by_bar.bar <= 1)).qj('by_bar == 0 & by_bar <= 1')

((foos_by_bar.bar == 0) | (foos_by_bar.bar == 1)).qj('by_bar == 0 | by_bar == 1')
((foos_by_bar.bar == 0) | (foos_by_bar.bar <= 1)).qj('by_bar == 0 | by_bar <= 1')

((foos_by_bar.bar == 0) ^ (foos_by_bar.bar == 1)).qj('by_bar == 0 ^ by_bar == 1')
((foos_by_bar.bar == 0) ^ (foos_by_bar.bar <= 1)).qj('by_bar == 0 ^ by_bar <= 1')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

foos.apply(len).qj('apply len')
foos.apply('keys').qj('apply keys')

None

foos = pist([pict(foo=i, bar=i % 3) for i in range(5)])
foos_by_bar = foos.bar.groupby()
(foos_by_bar.bar == 0).qj('by_bar == 0').pshape().qj('pshape').root().nonempty().qj('by_bar == 0 nonempty')
None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

foos.bar.qj('bar').root().baz.qj('baz')
foos[('bar','baz')].qj('bar,baz')
foos.bar.preduce_eq().qj('bar.pred=').root().qj('bar.pred=.root')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().baz.sortby_().root().qj('by_bar_baz')
by_bar_baz[('bar','baz')].qj('bar,baz')
by_bar_baz.bar.preduce_eq__().qj('bar.pred=').root().qj('bar.pred=.root')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

foos[:3].qj('foos[:3]').bar.qj('foos[:3].bar')
foos.bar[:3].qj('foos.bar[:3]').root().bar.qj('foos.bar[:3].root.bar')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).qj('sortby bar').groupby().qj('by_bar').baz.groupby().baz.qj('by_bar_baz.baz').sortby_().qj('by_bar_baz.baz.sortby').root().baz.qj('by_bar_baz.baz.sortby.root.baz').root().bar.qj('by_bar_baz.baz.sortby.root.bar').root()
by_bar_baz.qj('by_bar_baz').baz.qj('by_bar_baz.baz')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().baz.sortby_().root()
by_bar_baz.qj('by_bar_baz')

by_bar_baz.apply(len, pepth=0).qj('len')
by_bar_baz.apply(len, pepth=1).qj('len_')
by_bar_baz.apply(len, pepth=2).qj('len__ (actually on internal picts)')

by_bar_baz.plen().qj('plen')
by_bar_baz.plen(2).qj('plen 2')
by_bar_baz.plen(-1).qj('plen -1')

by_bar_baz.rlen().qj('rlen')
by_bar_baz.rlen(2).qj('rlen 2')
by_bar_baz.rlen(-1).qj('rlen -1')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().baz.sortby_().root()
by_bar_baz.qj('by_bar_baz')

by_bar_baz.pshape().qj('pshape')
by_bar_baz.pfill().qj('pfill')
by_bar_baz.pfill_().qj('pfill_')
by_bar_baz.pfill__().qj('pfill__')
by_bar_baz.pfill(-1).qj('pfill -1')
by_bar_baz.pfill(5).qj('pfill 5')
by_bar_baz.apply('pfill', -1, pepth=-1).qj('apply pfill(-1) pepth=-1 (useful for generating complete indices)')

qj(by_bar_baz.lfill(), 'lfill', l=lambda x: type(x))
qj(by_bar_baz.lfill_(), 'lfill_', l=lambda x: type(x))
qj(by_bar_baz.lfill__(), 'lfill__', l=lambda x: type(x))
qj(by_bar_baz.lfill(-1), 'lfill -1', l=lambda x: type(x))
qj(by_bar_baz.lfill(5), 'lfill 5', l=lambda x: type(x))
qj(by_bar_baz.apply('lfill', -1, pepth=-1), 'apply lfill(-1) pepth=-1 (useful for generating complete indices)', l=lambda x: type(x))

by_bar_baz.pleft().qj('pleft')
by_bar_baz.pleft_().qj('pleft_')
by_bar_baz.pleft__().qj('pleft__')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().baz.sortby_().root()
by_bar_baz.qj('by_bar_baz')

by_bar_baz.ungroup_().qj('bbb.ug_').ungroup().qj('bbb.ug_.ug')
by_bar_baz.ungroup(2).qj('bbb.ug^2')
by_bar_baz.ungroup(-1).qj('bbb.ug^-1')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().sortby_()
by_bar_baz.qj('by_bar_baz').values_like().qj('vals_like').values_like(11).qj('vals_like 11').root().values_like_(12).qj('vals_like_ 12').ungroup_().values_like('a').qj('ug.vals_like a').root().qj('root').values_like(by_bar_baz.foo + by_bar_baz.bar).qj('vals_like foo + bar')

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.groupby()
baz = by_bar.baz.np_().qj('np').sum().qj('baz sum')
max_baz = baz[baz.np().argmax().apply(np.expand_dims, -1).tolist().qj('baz.argmax')].qj('baz[argmax]')
(baz == max_baz).qj('baz == max_baz').foo.qj('final foos').root().bin = 13

_ = by_bar.qj('by_bar')
_ = by_bar.bin.groupby().qj('by_bar_bin')

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6

by_bar_baz = foos.bar.sortby(reverse=True).groupby().baz.groupby().sortby_().root()

(by_bar_baz.bar == 0).qj('by_bar == 0').pshape().qj('pshape').root().nonempty().qj('by_bar == 0 nonempty')
(by_bar_baz.bar == 0).nonempty(2).qj('by_bar == 0 nonempty^2')
(by_bar_baz.bar == 0).nonempty(-1).qj('by_bar == 0 nonempty^-1').nonempty().qj('by_bar == 0 nonempty^-1 nonempty')
None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.sortby(reverse=True).groupby()
baz = by_bar.baz.np_().qj('np').sum().qj('baz sum')
(baz == baz.np().max()).qj('baz == max_baz').foo.qj('final foos').root().bin = 13

foos.remix().qj('foos remix()')
foos.remix('foo', 'baz', new_bin=foos.bin * 13).qj('foos remix(*a, **kw)')

by_bar.remix().qj('by_bar remix()')
by_bar_rm = by_bar.remix('foo', 'baz', new_bin=by_bar.bin * 13).qj('by_bar remix(*a, **kw)')

by_bar_rm.baz.qj('bbr.baz').root_().qj('bbr.baz.root_')

_ = by_bar.qj('by_bar')
by_bar_bin = by_bar.bin.groupby().qj('by_bar_bin')

qj(by_bar_rm.pd(), 'bbr.pd')
foos.pd(index='foo')

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.groupby()
baz = by_bar.baz.np_().sum()
(baz == baz.np().max()).bin = 13

by_bar_bin = by_bar.qj('by_bar').bin.groupby().qj('by_bar_bin').bin.sortby__().root().qj('by_bar_bin.sort')
by_bar_bin.pd__().qj_('inner dataframes')

qj(by_bar_bin.ungroup_().qj('bb.ungroup').ungroup().qj('bb.ungroup.ungroup').pd(index='foo'), 'bb.ug.ug.pd')
qj(by_bar_bin.ungroup(2).qj('bb.ug^2').foo.sortby(key=lambda x: (x + 1) * ((x + 1) % 2) - (x + 1) * (x % 2), reverse=True).qj('bb.ug^2.sortby(evens->0->odds)').root().pd(index='foo'), 'bb.ug.ug.sortby.pd')

foos.pd(index='foo')

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.groupby()
baz = by_bar.baz.np_().sum()
(baz == baz.np().max()).bin = 13

by_bar.bin.groupby().qj('by_bar_bin').baz.groupby().qj('bb.baz.gb').ungroup(-1).qj('ungroup all').pd()

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.groupby()
baz = by_bar.baz.np_().sum()
(baz == baz.np().max()).bin = 13
df = qj(by_bar.ungroup().pd(), 'by_bar')
qj(df[(df.bar == 0) & (df.bin == -1)], 'df[bar == 0 and bin == -1]')

qj(((by_bar.qj('by_bar').bar == 0).qj('bar == 0', l=lambda x: len(x)) & (by_bar.bin == -1).qj('bin == -1', l=lambda x: len(x))).qj('bar == 0 and bin == -1').ungroup().pd(), 'df')
qj(((by_bar.bar == 0) | (by_bar.bin == -1)).qj('bar == 0 or bin == -1').ungroup().pd(), 'df')
qj(((by_bar.bar == 0) ^ (by_bar.bin == -1)).qj('bar == 0 xor bin == -1').ungroup().pd(), 'df')
qj(((by_bar.bar == 0) & (by_bar.bar == 0)).qj('bar == 0 and bar == 0').ungroup().pd(), 'df')

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 + (foos.bar == 0).foo
(foos.bar == 1).baz = 6
foos.bin = -1

by_bar = foos.bar.groupby()
baz = by_bar.baz.np_().sum()
(baz == baz.np().max()).bin = 13

by_bar_bin = by_bar.bin.groupby().qj('by_bar_bin')
by_bar_bin.foo.qj__('CORRECT: foos by_bar_bin')
by_bar_bin.foo.apply(qj, 'SHOULD MATCH: foos by_bar_bin', pepth=1)
by_bar_bin.foo.apply_(qj, 'SHOULD MATCH_: foos by_bar_bin')
by_bar_bin.foo.apply(qj, 'PAINFUL *args bin=' + by_bar_bin.bar.apply(str, pepth=2) + ' baz=' + by_bar_bin.baz.apply(str, pepth=2), pepth=1)
by_bar_bin.foo.apply(qj, s='PAINFUL **kwargs bin=' + by_bar_bin.bar.apply(str, pepth=2) + ' baz=' + by_bar_bin.baz.apply(str, pepth=2), pepth=1)

by_bar_bin.foo.apply(qj, 'NICE *a bin=' + by_bar_bin.bar.pstr() + ' baz=' + by_bar_bin.baz.pstr(), pepth=1)
by_bar_bin.foo.apply(qj, s='NICE **kw bin=' + by_bar_bin.bar.pstr() + ' baz=' + by_bar_bin.baz.pstr(), pepth=1)

None

foos = pist([pict(foo=i, bar=i % 2) for i in range(5)])
(foos.bar == 0).baz = 3 - ((foos.bar == 0).foo % 3)
(foos.bar == 1).baz = 6
foos.qj('foos')

(foos == foos[:1]).qj('foos == foos[:1]')
(foos != foos[:1]).qj('foos != foos[:1]')

(foos == foos[:2]).qj('foos == foos[:2]')
(foos != foos[:2]).qj('foos != foos[:2]')

(foos == foos[1:2]).qj('foos == foos[1:2]')
(foos != foos[1:2]).qj('foos != foos[1:2]')

(foos == foos[1:]).qj('foos == foos[1:]')
(foos != foos[1:]).qj('foos != foos[1:]')

(foos.foo >= foos[:1].foo).qj('foos.foo >= foos[:1].foo')
(foos.foo <= foos[:1].foo).qj('foos.foo <= foos[:1].foo')

(foos.foo >= foos[1:2].foo).qj('foos.foo >= foos[1:2].foo')
(foos.foo <= foos[1:2].foo).qj('foos.foo <= foos[1:2].foo')

(foos.foo >= foos[1:4].foo).qj('foos.foo >= foos[1:4].foo')
(foos.foo <= foos[1:4].foo).qj('foos.foo <= foos[1:4].foo')

(foos.foo > foos[:1].foo).qj('foos.foo > foos[:1].foo')
(foos.foo < foos[:1].foo).qj('foos.foo < foos[:1].foo')

(foos.foo > foos[1:2].foo).qj('foos.foo > foos[1:2].foo')
(foos.foo < foos[1:2].foo).qj('foos.foo < foos[1:2].foo')

(foos.foo > foos[1:4].foo).qj('foos.foo > foos[1:4].foo')
(foos.foo < foos[1:4].foo).qj('foos.foo < foos[1:4].foo')

by_bar_baz = foos.bar.sortby().groupby().baz.groupby().sortby_().qj('by_bar_baz')

(by_bar_baz == by_bar_baz[:1]).qj('by_bar_baz == by_bar_baz[:1]')
(by_bar_baz != by_bar_baz[:1]).qj('by_bar_baz != by_bar_baz[:1]')

(by_bar_baz == by_bar_baz[:2]).qj('by_bar_baz == by_bar_baz[:2]')
(by_bar_baz != by_bar_baz[:2]).qj('by_bar_baz != by_bar_baz[:2]')

(by_bar_baz.foo >= by_bar_baz[:1].foo).qj('by_bar_baz.foo >= by_bar_baz[:1].foo')
(by_bar_baz.foo <= by_bar_baz[:1].foo).qj('by_bar_baz.foo <= by_bar_baz[:1].foo')

(by_bar_baz.foo >= by_bar_baz[:2].foo).qj('by_bar_baz.foo >= by_bar_baz[:2].foo (switches to element-wise comparison)')
(by_bar_baz.foo <= by_bar_baz[:2].foo).qj('by_bar_baz.foo <= by_bar_baz[:2].foo (switches to element-wise comparison)')

None

"""# agents

If you want to follow along at home, you can copy this notebook and run the following command on your workstation from a directory where you're happy to download 30,000 small files:

```bash
fileutil --parallel_copy=100 cp /cns/ok-d/home/iansf/vale-project/rl_predict/agents/*minimal.p .
```

You'll need to change `BASE_SAVE_DIR` and `AGENTS_DIR` below to the path you copied them to.  Then, running a local colab server on your workstation, this notebook should run.
"""

BASE_SAVE_DIR = '/usr/local/google/home/iansf/rl_predict/data'
AGENTS_DIR = '{BASE_SAVE_DIR}/agents10'.format(**locals())
CHARTS_DIR = '{BASE_SAVE_DIR}/charts'.format(**locals())

NUM_PROCESSES = 24

def charts_base(tag):
  return os.path.join(CHARTS_DIR, tag)

def agent_id(agent_file):
  if agent_file.endswith('_minimal.p'):
    return os.path.basename(agent_file).split('_')[0]
  return hashlib.md5(agent_file).hexdigest()

if not 'cached_agents' in globals():
  cached_agents = {}
cached_agents = {}

@qj(time=1000)
def load_agent_file(path):
  with open(path, 'r') as f:
    try:
      agent = f.read()
    except Exception, e:  # pylint: disable=broad-except
      qj('Failed to load agent from %s due to error: %s' % (path, str(e)))
      agent = ''
  return agent, path


@qj(time=1000)
def unpickle(agent_and_path):
  agent, path = agent_and_path
  return pickle.loads(agent), path


def fix_subkeys(stats):
  for k in stats:
    if isinstance(stats[k], dict):
      stats[k] = defaultpict(list, **stats[k])
      for sk in stats[k]:
        stats[k][sk] = list(stats[k][sk])


def get_agent(agent_file, retry=False):
  a_id = agent_id(agent_file)
  if a_id in cached_agents and (not retry or cached_agents[a_id] is not None):
    return cached_agents[a_id]
  local_file = os.path.join(AGENTS_DIR, '%s_minimal.p' % a_id)
  agent = load_agent_local(local_file)
  return clean_agent(agent)


@qj(time=1000)
def clean_agent(agent_and_path):
  agent, path = agent_and_path
  if agent is None:
    return None

  agent = pict(**agent)
  agent.path = path

  path_comps = agent.checkpoint_dir.split('/')
  agent.mid = path_comps[9] + '-' + path_comps[2] if len(path_comps) > 9 else '<missing_mid>'
  agent.exp_name = path_comps[7] if len(path_comps) > 7 else '<missing_exp_name>'
  try:
    agent.a_id = int(path_comps[-2])
  except Exception:
    agent.a_id = np.nan
  agent.winner = False
  
  agent.stats = defaultpict(lambda: defaultpict(list), **agent.stats)
  fix_subkeys(agent.stats)

  agent.stats.test = agent.stats.test_stoch
  if len(agent.stats.test.wins):
    wins = agent.stats.test.wins[-1]
    del agent.stats.test.wins[:]
    agent.stats.test.wins.append(wins)
  
  agent.original_settings = pict(**agent.original_settings)
  agent.num_params = int(agent.num_params)
  
  if (len(agent.stats.test_stoch.total_reward) < 100
      or ('ww_multi' in agent.checkpoint_dir and len(agent.stats.test_stoch_solo.total_reward) < 100)
      or agent.mid == '<missing_mid>'
      or agent.exp_name == '<missing_exp_name>'
      or np.isnan(agent.a_id)):
    qj((agent.exp_name, agent.name, agent.a_id), 'Agent %s had missing data (%d stoch rewards, %d solo rewards)' % (agent.unique_id, len(agent.stats.test_stoch.total_reward), len(agent.stats.test_stoch_solo.total_reward)))
    cached_agents[agent.unique_id] = None
    return None

  if 'recording_dirs' not in agent:
    agent.recording_dirs = []

  agent.update(agent.original_settings)
  del agent.original_settings

  cached_agents[agent.unique_id] = agent
  if len(cached_agents) % 1000 == 0:
    qj(len(cached_agents), 'Num loaded agents')
  return cached_agents[agent.unique_id]


# Loads ww_act_two in 16 1/2 minutes
def get_agents(agent_files):
  pool = Pool(NUM_PROCESSES)
  files_per_process = [[] for _ in range(NUM_PROCESSES)]
  for i, f in enumerate(agent_files):
    files_per_process[i % NUM_PROCESSES].append(f)

  agent_strs_per_process = ((load_agent_file(f) for f in files) for files in files_per_process)
  agent_pickles_per_process = (pool.map(unpickle, gen_agents) for gen_agents in agent_strs_per_process)
  return (clean_agent(agent) for agents in agent_pickles_per_process for agent in agents)


# Runs in 20 minutes on ww_act_two
# def get_agents(agent_files):
#   pool = Pool(NUM_PROCESSES)
#   agent_strs = (load_agent_file(f) for f in agent_files)
#   agent_pickles = (agent for agent in pool.map(unpickle, agent_strs))
#   return (clean_agent(agent) for agent in agent_pickles)

"""### trash"""

# def get_agent(agent_file, retry=False):
#   a_id = agent_id(agent_file)
#   if a_id in cached_agents and (not retry or cached_agents[a_id] is not None):
#     return cached_agents[a_id]
#   local_file = os.path.join(AGENTS_DIR, '%s_minimal.p' % a_id)
#   if not gfile.Exists(local_file):
#     qj('Copying agent from %s to %s' % (agent_file, local_file))
#     gfile.Copy(agent_file, local_file, overwrite=True)
#   agent = U.load_agent_local(local_file)
#   if agent is None:
#     return None

#   def fix_subkeys(stats):
#     for k in stats:
#       if isinstance(stats[k], dict):
#         stats[k] = defaultpict(list, **stats[k])
#         for sk in stats[k]:
#           stats[k][sk] = list(stats[k][sk])

#   agent = pict(**agent)

#   path_comps = agent.checkpoint_dir.split('/')
#   agent.mid = path_comps[9] + '-' + path_comps[2] if len(path_comps) > 9 else '<missing_mid>'
#   agent.exp_name = path_comps[7] if len(path_comps) > 7 else '<missing_exp_name>'
#   try:
#     agent.a_id = int(path_comps[-2])
#   except Exception:
#     agent.a_id = np.nan
#   agent.winner = False
  
#   agent.stats = defaultpict(lambda: defaultpict(list), **agent.stats)
#   fix_subkeys(agent.stats)

#   agent.stats.test = agent.stats.test_stoch
#   if len(agent.stats.test.wins):
#     wins = agent.stats.test.wins[-1]
#     del agent.stats.test.wins[:]
#     agent.stats.test.wins.append(wins)
  
#   agent.original_settings = pict(**agent.original_settings)
#   agent.num_params = int(agent.num_params)
  
#   if (len(agent.stats.test_stoch.total_reward) < 100
#       or ('ww_multi' in agent.checkpoint_dir and len(agent.stats.test_stoch_solo.total_reward) < 100)
#       or agent.mid == '<missing_mid>'
#       or agent.exp_name == '<missing_exp_name>'
#       or np.isnan(agent.a_id)):
#     qj((agent.exp_name, agent.name, agent.a_id), 'File %s had missing data (%d stoch rewards, %d solo rewards)' % (agent_file, len(agent.stats.test_stoch.total_reward), len(agent.stats.test_stoch_solo.total_reward)))
#     cached_agents[a_id] = None
#     return None
  
#   if 'recording_dirs' not in agent:
#     agent.recording_dirs = []
  
#   agent.update(agent.original_settings)
#   del agent.original_settings
  
#   cached_agents[a_id] = agent
#   if len(cached_agents) % 1000 == 0:
#     qj(len(cached_agents), 'Num loaded agents')
#   return cached_agents[a_id]

# agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

# agents = pist([a for a in map(lambda x: get_agent(x, retry=False), agent_files) if a is not None])
# qj(len(agents))

# agents = (pist(map(lambda x: get_agent(x, retry=False), agent_files)) != None)
# agents.plen().qj('Num agents')

# agents.wins = 0
# wins = (((agents.mid.groupby().apply(len) == 3).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
# (wins == wins.np().max()).winner = True
# wins.root().wins = wins
# wins[:2].qj('first 2 wins')

# ww_zs_agents = (wins.root().exp_name == 'ww_zs').nonempty()
# ww_zs_agents.num_future_action_predictions = ww_zs_agents.num_future_action_predictions.np().max()

# None

# @qj(time=10)
# def how_long():
#   agents.stats.test.total_reward

# [how_long() for _ in range(10)]
# _ = qj(len(agents))

# @qj(time=10)
# def how_long_lame():
#   lame_agents = []
#   for a in agents:
# #     lame_agents.append(a.stats.test.total_reward)
#     lame_agents.append(a['stats']['test']['total_reward'])

# [how_long_lame() for _ in range(10)]
# _ = qj(len(agents))

# _ = ((agents.exp_name == 'ww_beta_multi') & (agents.winner == True)).exp_name.qj('exp_name').root().name.qj('name')

"""# analyze data

### filter and clean data
"""

agents = (pist(map(lambda x: get_agent(x, retry=False), agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
agents.runner_up = False
wins = (((agents.mid.groupby().apply(len) == 3).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins
((wins.root().winner == False) & (wins != wins.np().min())).runner_up = True

wins[:2].qj('First 2 wins').root().winner.qj('winners').root().runner_up.qj('runners up')

ww_zs_agents = (wins.root().exp_name == 'ww_zs').nonempty()
ww_zs_agents.num_future_action_predictions = ww_zs_agents.num_future_action_predictions.np().max()

ww_gs_agents = (wins.root().exp_name == 'ww_gs_pred').nonempty()
ww_gs_agents.num_future_action_predictions = ww_gs_agents.num_future_action_predictions.np().max()

((agents.winner == True).wins < 33.3).wins.qj('illegal wins').root().mid.qj('bad mids')

None

"""### logging and plotting helpers"""

def log_wins(agents, field, index='a_id', reverse=False):
  by = agents[field].qj(field, b=0).sortby(reverse=reverse).groupby()[index].sortby_().groupby()
  by_labels = str(field) + ' ' + by.root().ungroup(-1)[field].pstr().preduce_eq() + ' win counts (%)' + str(by[index].ungroup(-1).preduce_eq())
  by.plen__().apply(qj, by_labels, l=lambda y: y / float(np.sum(y))) #.root().remix__('a_id', field, 'wins').pd__().qj_('dataframes')
  return by


def plot(y, x=None, label='', title=None, remaining=0, mean_std=True, min_max=False, axis=0, figsize=None, legend_loc='best', filename=None, xlabel=None, ylabel=None, ylim=None, legend_args=None, *a, **kw):
  current_title = plt.gca().get_title()
  if current_title and title != current_title:
    finalize_plot(**locals())

  _, labs = plt.gca().get_legend_handles_labels()
  if label in labs:
    label = ''

  if mean_std:
    y = np.array(y)
    mean = np.mean(y, axis=axis)
    std = np.std(y, axis=axis)
    if x is None:
      x = np.arange(len(mean))
    elif len(x) != len(mean):
      x = np.array(x).mean()
    line = plt.plot(x, mean, label=label, *a, **kw)[0]

    if min_max:
      min_ = np.min(y, axis=axis)
      max_ = np.max(y, axis=axis)
      plt.plot(x, min_, label=label + ' min/max', alpha=0.25, color=line.get_color())[0]
      plt.plot(x, max_, alpha=0.25, color=line.get_color())[0]

      plt.fill_between(x, min_, max_, alpha=0.05, color=line.get_color())

    plt.fill_between(x, mean - std, mean + std, alpha=0.25, color=line.get_color())
    
  else:
    if x is None:
      x = np.arange(len(y))
    line = plt.plot(x, y, label=label, *a, **kw)[0]

  if title is not None:
    plt.gca().set_title(title)

  if remaining == 0:
    finalize_plot(**locals())
  
  return line.get_color()


def plot_with_baseline(y, baseline, x=None, label='', baseline_label='', title=None, remaining=1, mean_std=True, min_max=False, *a, **kw):
  color = plot(y, x=x, label=label, title=title, remaining=1, mean_std=mean_std, min_max=min_max, *a, **kw)
  kw.update(color=color)
  plot(np.ones(np.array(y).shape[-1]) * baseline, x=x, label=baseline_label, title=title, remaining=remaining, mean_std=False, min_max=False, ls='--', *a, **kw)


def plot_hist(x, label='', secondary_x=None, secondary_label='', title=None, figsize=None, legend_loc='best', filename=None, xlabel=None, ylabel=None, ylim=None, legend_args=None, *a, **kw):
  label = list(label)

  counts, bins, bars = plt.hist(x, label=label, *a, **kw)

  if secondary_x is not None:
    counts = counts if isinstance(counts, list) else list(counts)
    bars = bars if isinstance(bars, list) else list(bars)
    bars = pist(bars).get_children().apply(pist).uproot()
    secondary_label = list(secondary_label)
    alpha = kw.pop('alpha', 0.5) * 0.5
    kw.update(bins=(kw.pop('bins', None) and 0) or bins)
    for i in range(len(counts)):
      _, _, new_bars = plt.hist(secondary_x, bottom=counts[i], *a, **kw)
      new_bars = pist(new_bars).get_children().apply(pist).uproot()
      new_bars[i:i+1].apply('set_fc', bars[i:i+1].get_fc().ungroup(-1).preduce_eq()).root().set_alpha(alpha).root().ungroup(-1)[:1].set_label(secondary_label[i])
      (new_bars != new_bars[i:i + 1]).set_visible(False).root().set_label(None)
    # Sadly, this doesn't work when there are invisible collections, according to the docs, which is the case here.
    plt.gca().relim(visible_only=True)

  if title is not None:
    plt.gca().set_title(title)
  
  finalize_plot(**locals())


def finalize_plot(figsize=(16, 12), legend_loc='best', filename=None, xlabel=None, ylabel=None, ylim=None, legend_args=None, **_):
  legend_args = {} if legend_args is None else legend_args
  if legend_loc is not None:
    legend_args.update(loc=legend_loc)
  if legend_args:
    plt.gca().legend(**legend_args)
  if figsize is not None:
    plt.gcf().set_size_inches(*figsize)
  if xlabel is not None:
    plt.xlabel(xlabel)
  if ylabel is not None:
    plt.ylabel(ylabel)
  if ylim is not None:
    plt.ylim(*ylim)
  if filename:
    plt.gcf().tight_layout(pad=0)
    with gfile.Open(filename, 'w') as f:
      plt.gcf().savefig(f, dpi='figure')
  plt.show()

"""### other helpers"""

def normalize(x):
  x = np.array(x)
  return x / np.sum(x)


def softermax(x, b=np.e):
  sm = np.array(x).astype(np.float64)
  sm = b ** (sm - np.max(sm, axis=-1, keepdims=True))
  return (sm * (1.0 / np.sum(sm, axis=-1, keepdims=True))).astype(x.dtype)


def set_loss_scale_factors(agents):
  for num_future_action_predictions in agents.num_future_action_predictions.preduce_eq():
    if num_future_action_predictions == 1:
      (agents.num_future_action_predictions == num_future_action_predictions).loss_scale_factors = [1.0]
      continue
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == 'const')).loss_scale_factors = [1.0] + [1.0 / float(num_future_action_predictions - 1) for _ in range(1, num_future_action_predictions)]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == 'linear')).loss_scale_factors = [1.0] + [dropoff for dropoff in normalize(np.arange(1.0, num_future_action_predictions))]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == '1m_linear')).loss_scale_factors = [1.0] + [dropoff for dropoff in normalize(np.arange(num_future_action_predictions - 1.0, 0, -1))]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == 'quad')).loss_scale_factors = [1.0] + [dropoff for dropoff in normalize(np.arange(1.0, num_future_action_predictions) ** 2)]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == '1m_quad')).loss_scale_factors = [1.0] + [dropoff for dropoff in normalize(np.arange(num_future_action_predictions - 1.0, 0, -1) ** 2)]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == 'exp')).loss_scale_factors = [1.0] + [dropoff for dropoff in softermax(np.arange(1.0, num_future_action_predictions, dtype=np.float64))]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == '1m_exp')).loss_scale_factors = [1.0] + [dropoff for dropoff in softermax(np.arange(num_future_action_predictions - 1, 0, -1, dtype=np.float64))]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == 'inv_exp')).loss_scale_factors = [1.0] + [dropoff for dropoff in softermax(1.0 / np.arange(1, num_future_action_predictions, dtype=np.float64))]
    ((agents.num_future_action_predictions == num_future_action_predictions) & (agents.future_prediction_losses_dropoff == '1m_inv_exp')).loss_scale_factors = [1.0] + [dropoff for dropoff in softermax(1.0 / np.arange(num_future_action_predictions - 1, 0, -1, dtype=np.float64))]

"""### ww_baseline_single, ww_future_single

These experiments are the single-agent baselines in WaterWorld.  The grid search is over the policy learning rate (plr), value learning rate (vlr), policy layer widths (plw, two values separated by '\_'), value layer width (vlw), and the number of predictions (nfap, num future action predictions, which corresponds to $k$ in the paper).

The charts show the means, stds, mins, and maxes for each hparam setting.  The y axis is the mean total reward at test time (averaged across 100 test episode for each agent).  The x axis is $k$, ranging from 0 (normal policy gradient) to 10 by increments of 2.  Each plot breaks out the three different value layer widths (64, 128, 256) for the other settings specified in the title of the plot.

The end result is that the best learning rates are plw=0.01, vlw=0.005, across all future prediction values, and everyone is able to perform well with both small ('40_20') and large ('100_40') policy networks, and all three sizes of value networks at those learning rates.  Performance at those settings is basically flat for all different numbers of predictions, although the best performing network is at 10 predictions, by a noticeable margin.

For these plots each hparam setting was run 10 times.  However, the predictive agents also varied across an additional hparam, the dropoff function that produces the $\kappa$ values ($\beta$ was set to $1/k$ in all cases).  Five dropoff functions were tested, but they aren't broken out in these charts, so for $k>0$, each point in the chart represents 50 different agents.

The second cell groups the plots by (plr, vlr, plw, vlw) and breaks out the different dropoff functions.  There are a lot of charts, so I pulled the best one out and put it first.  For both const and inv_exp at the globally best hparams, there is a clear performance improvement from $k=0$ through $k=10$.

I also should generate the mutual information through time charts for the baselines, but I would need to rerun the tests in order to generate that data.
"""

qj(len(agents), 'agents')
bfagents = ((agents.exp_name == 'ww_baseline_single') | (agents.exp_name == 'ww_future_single'))
qj(len(bfagents), 'bfagents')

fields = ('policy_learning_rate', 'value_learning_rate', 'policy_layer_widths', 'value_layer_width')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

by.num_future_action_predictions.ungroup(-1).preduce_eq().qj('nfap')

rmx = by.ungroup(-1).remix('num_future_action_predictions', reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields)[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx.reward.np_().qj('rew', b=0).apply(
    plot,
    x=rmx.num_future_action_predictions.np_().mean().qj('nfap', b=0) - 1,
    label=rmx.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='solo play rewards (plr,vlr,plw)=' + rmx.ungroup(-1)[fields].preduce_eq().root()['policy_learning_rate', 'value_learning_rate', 'policy_layer_widths'].pstr().qj('params', b=0),
    remaining=rmx.ungroup(-1)[fields].preduce_eq().pleft().qj('pleft', b=0),
    min_max=True,
    c=plt.rcParams['axes.color_cycle'][:6] * (len(rmx) // 6),
    alpha=0.75,
    pepth=0)

(rmx.ungroup(-1)[fields] == (0.001, 0.0005, '100_40', 64)).pd()

qj(len(agents), 'agents')
# fsagents = (agents.exp_name == 'ww_future_single')
fsagents = ((agents.exp_name == 'ww_baseline_single') | (agents.exp_name == 'ww_future_single'))
qj(len(fsagents), 'fsagents')

fields = ('policy_learning_rate', 'value_learning_rate', 'policy_layer_widths', 'value_layer_width', 'future_prediction_losses_dropoff')

by = fsagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

by.num_future_action_predictions.ungroup(-1).preduce_eq().qj('nfap')

rmx = by.ungroup(-1).remix('num_future_action_predictions', reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields)

rmx.plen().qj('rmx len')

# Duplicate nfap=1 agents to pretend that we ran them with all of the difference dropoff functions, even though they have no impact when nfap=1.
rmx.extend(((rmx.num_future_action_predictions == 1) & (rmx.future_prediction_losses_dropoff == '1m_inv_exp')).copy().uproot().update(future_prediction_losses_dropoff='1m_linear'))
rmx.extend(((rmx.num_future_action_predictions == 1) & (rmx.future_prediction_losses_dropoff == '1m_inv_exp')).copy().uproot().update(future_prediction_losses_dropoff='const'))
rmx.extend(((rmx.num_future_action_predictions == 1) & (rmx.future_prediction_losses_dropoff == '1m_inv_exp')).copy().uproot().update(future_prediction_losses_dropoff='linear'))
rmx.extend(((rmx.num_future_action_predictions == 1) & (rmx.future_prediction_losses_dropoff == '1m_inv_exp')).copy().uproot().update(future_prediction_losses_dropoff='inv_exp'))

rmx.plen().qj('rmx len')

rmx_best = (rmx[fields[:-1]] == (0.01, 0.005, '40_20', 256))
rmx_best = (rmx_best.future_prediction_losses_dropoff == 'const')[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()
# rmx_best = ((rmx_best.future_prediction_losses_dropoff == 'const') | (rmx_best.future_prediction_losses_dropoff == 'inv_exp'))[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()
rmx = rmx[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx_best.reward.np_().apply(
    plot,
    x=rmx_best.num_future_action_predictions.np_().mean() - 1,
    label=rmx_best.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='best solo play rewards (plr,vlr,plw,vlw)=' + rmx_best.ungroup(-1)[fields].preduce_eq().root()[fields[:-1]].pstr(),
    remaining=rmx_best.ungroup(-1)[fields].preduce_eq().pleft(),
    min_max=True,
#     c=pist(plt.rcParams['axes.color_cycle'])[[3,4]],
    alpha=0.75,
    pepth=0)


rmx.reward.np_().apply(
    plot,
    x=rmx.num_future_action_predictions.np_().mean() - 1,
    label=rmx.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='solo play rewards (plr,vlr,plw,vlw)=' + rmx.ungroup(-1)[fields].preduce_eq().root()[fields[:-1]].pstr(),
    remaining=rmx.ungroup(-1)[fields].preduce_eq().pleft(),
    min_max=True,
    c=plt.rcParams['axes.color_cycle'][:5] * (len(rmx) // 5),
    alpha=0.75,
    pepth=0)


(rmx.ungroup(-1)[fields[:-1]] == (0.01, 0.005, '40_20', 64)).pd()

"""### ww_multi

These experiments are multi-agent runs in WaterWorld.  In all cases, there are three agents with identical hparams apart from two booleans that determin whether the agent is predictive or not and whether the agent trains its $k$ predictive layers.

The hparams vary across policy layer widths ('40_20', '100_40'), value layer width (64, 128, 256), and dropoff function (constant, linear, 1 - linear, inverse exponential, 1 - inverse exponential).

Each experiment is a tournament of 100 testing episodes.  The winner of each episode is determined by which agent has the highest total reward and accounts for ties.  The winner of each tournament is the agent with the highest number of episode wins, and also accounts for ties (all agents with the highest number of episode wins are marked as the winner of the tournament).

Each of the hparam settings was run 10 times.

Agent 0 (blue) is predictive with $k=10$.

Agent 1 (green) is not predictive, but trains its $k$ predictive layers anyway so that it can compute its mutual information through time.

Agent 2 (red) is not predictive and does not use the KL loss to train the $k$ predictive layers, so the classifiers at each of the predictive layers are based off of a fixed random function of the observations.  This agent is a debugging baseline for myself and won't appear in the paper.

The log messages show statistics per agent for wins and strong second place finishes combined for the whole experiment, for each individual hparam, for the layer width hparams together, and finally for all three hparams together.  A strong second place finish is defined as having won more than 1/3 of the episodes.  The numbers when only counting wins are slightly stronger in favor of Agent 0.

The first set of charts show the mutual information between the observations at timestep $t$ and the actions taken at timestep $t+k$.  The y axis is nats.  The x axis is $k$.  The dashed lines are the mean entropies for the agents, which is the upper bound of the mutual information.  Maximum entropy for these agents would be ~1.6 nats (ln(5)), but the fifth action (do nothing) is generally less useful than the other four (move in the cardinal directions), so observed entropies are lower.

The second set of charts are bar charts showing the win counts for each of the different agents, with second place counts stacked on top.

The primary conclusion from these experiments is that the inverse exponential function gives the best mutual information retention as well as the largest performance gap between predictive and non-predictive agents at the optimal hparam settings (plw='100_40', vlw=128 or 256).  Those agents win about 45% of the tournaments, with the equivalent non-predictive agents each getting about 27.5% of the wins.  This result should be significant, as it is aggregated from 80 tournaments.  However, it is also the case that the linear dropoff function gave the most consistent advantage to the predictive agent, with the predictive agent having the most tournament wins at all six hparam settings, and winning over 40% of the time for 5 of the six.  The linear dropoff function does quite poorly at mutual information, though, hardly maintaining any advantage over the non-predictive agents, and having a sharp drop between 0 and 1 predictions.  I'm not sure how to explain that result, honestly.  The entropy drops for linear agents by about the same amount as for inverse exponential agents, so the predictive training is changing the agents' behaviors, but they don't end up being very predictable, in spite of the entropy drop.

Also note that these runs were started before the baseline runs were finished, so I used a slightly suboptimal value learning rate of 0.001 rather than 0.005.  The final set of multi-agent experiments are running now, with the ideal learning rates and only two agents per tournament.
"""

qj(len(agents), 'agents')
magents = (agents.exp_name == 'ww_multi')
qj(len(magents), 'magents')

magents = ((magents.winner == True) | ((magents.runner_up == True) & (magents.wins > 33.3333)))
qj(len(magents), 'first place and strong second place')

# magents = (magents.winner == True)
# qj(len(magents), 'winners')

log_wins(magents, ())

log_wins(magents, 'policy_layer_widths')
log_wins(magents, 'value_layer_width')
log_wins(magents, 'future_prediction_losses_dropoff')

log_wins(magents, ('policy_layer_widths', 'value_layer_width'))
by = log_wins(magents, ('future_prediction_losses_dropoff', 'policy_layer_widths', 'value_layer_width'))

winners = (by.winner == True)
runners = (by.runner_up == True)

# winners.stats.test.policy_mutual_information_through_time_self.apply(
#     plot_with_baseline,
#     winners.stats.test.policy_entropy_self.np_().mean(),
#     label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
#     baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
#     title='hparams:' + winners.ungroup(-1)[('future_prediction_losses_dropoff', 'policy_layer_widths', 'value_layer_width')].preduce_eq().pstr().qj('hparams'),
#     remaining=winners.pleft(),
#     mean_std=False,
#     c=plt.rcParams['axes.color_cycle'][:3],
#     alpha=0.7,
#     pepth=2)

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='hparams:' + winners.ungroup(-1)[('future_prediction_losses_dropoff', 'policy_layer_widths', 'value_layer_width')].preduce_eq().pstr().qj('hparams'),
    remaining=winners.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
    c=plt.rcParams['axes.color_cycle'][:3],
    pepth=1)

winners.wins.join().apply(
    plot_hist,
    label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' wins',
    secondary_x=runners.wins,
    secondary_label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' 2nd places',
    title='hparams:' + by.ungroup(-1)[('future_prediction_losses_dropoff', 'policy_layer_widths', 'value_layer_width')].preduce_eq().pstr(),
    alpha=0.5, range=(30, 80),
    pepth=1)


by.ungroup(-1).remix('a_id', 'name', 'exp_name', 'winner', 'runner_up', 'wins').a_id.sortby().root().pd(index='name').head(1000)

"""### ww_beta_multi

These experiments do a grid search over $\beta$ (the weighting term for the sum of the KLs) for $k=10$.  The other hparams are set to the best values found previously (plr=0.01, vlr=0.005, plw='100_40', vlw=256, inverse exponential dropoff function).

The log messages and plots are the same as for ww_multi, above, except that the first set of charts shows the mutual information for each agent individually, which accentuates that the tournament wins and strong second places are much fewer for Agent 0 in many of these runs.

Each set of hparams was run in 50 tournaments.

The conclusion here is that $\beta=1/k$ is optimal, at least for $k=10$ and inverse exponential dropoff.  It's also nice to see how sweeping $\beta$ from high to low shows the agents going from more to less predictable.  There's also a pretty clear signal where the predictive agent's performance in aggregate is much worse than the non-predictive agents' when $\beta$ is too high (the agent is too predictable and doesn't care enough about maximizing reward), but becomes stronger as $\beta$ approaches 0.1, and then drops back toward chance as $\beta$ becomes too low to provide an advantage to the predictive agent.

We were also curious about the fact that increasing $\beta$ results in Agent 1 (the non-predictive agent) showing much better mutual information as well, even though it's still significantly worse than Agent 0.  The hypotheses for why that might be are:
 - Having a strongly predictive agent that the other agents are cotrained with influences their training and makes them more predictable as well.
 - The larger $\beta$ causes the optimizer to make the prediction superstructure more optimal, even though those adjustments don't change the weights of the actual policy network.
 - There's a bug, and Agent 1 is actually leaking predictive information to the policy.  I ruled this out by inspecting the gradients of the policy network directly and verifying that they are 0 when I set the base policy gradient loss to 0.
 
I look at this question some more below in ww_multi_sfpg{0|1}.

I've added a chart showing how the mutual information changes with $\beta$ for both predictive and normal agents.
"""

qj(len(agents), 'agents')
bagents = (agents.exp_name == 'ww_beta_multi')
qj(len(bagents), 'bagents')

# bagents = ((bagents.winner == True) | ((bagents.runner_up == True) & (bagents.wins > 33.3333)))
# qj(len(bagents), 'first place and strong second place')

bagents = (bagents.winner == True)
qj(len(bagents), 'winners')

log_wins(bagents, ())
by = log_wins(bagents, 'future_prediction_losses_scale_factor', reverse=True)

winners = (by.winner == True)
runners = (by.runner_up == True)

sns.set_palette(sns.color_palette('Set2', 6))

# winners.stats.test.policy_mutual_information_through_time_self.apply(
#     plot_with_baseline,
#     winners.stats.test.policy_entropy_self.np_().mean(),
#     label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
#     baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
#     title='beta=' + winners.future_prediction_losses_scale_factor.pstr(),
#     remaining=winners.pleft(),
#     mean_std=False,
#     c=plt.rcParams['axes.color_cycle'][:3],
#     alpha=0.7,
#     pepth=2)

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='beta=' + winners.ungroup(-1).future_prediction_losses_scale_factor.preduce_eq().pstr(),
    remaining=winners.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
    c=plt.rcParams['axes.color_cycle'][:3],
    pepth=1)

# winners.wins.join().apply(
#     plot_hist,
#     label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' wins',
#     title='beta=' + by.ungroup(-1).future_prediction_losses_scale_factor.preduce_eq().pstr(),
#     alpha=0.5, range=(30, 80),
#     pepth=1)

# winners.wins.join().apply(
#     plot_hist,
#     label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' wins',
#     secondary_x=runners.wins,
#     secondary_label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' 2nd places',
#     title='beta=' + by.ungroup(-1).future_prediction_losses_scale_factor.preduce_eq().pstr(),
#     alpha=0.5, range=(30, 80),
#     pepth=1)


by.ungroup(-1).remix('a_id', 'name', 'exp_name', 'future_prediction_losses_scale_factor', 'num_future_action_predictions', 'winner', 'wins').a_id.sortby().root().pd(index='name').head(1000)
None

qj(len(agents), 'agents')
bagents = (agents.exp_name == 'ww_beta_multi')
qj(len(bagents), 'bagents')

bagents = ((bagents.a_id == 0) | (bagents.a_id == 1))

# bagents = (bagents.winner == True)
# qj(len(bagents), 'winners')

log_wins(bagents, ())
by = log_wins(bagents, 'future_prediction_losses_scale_factor', reverse=True)

a0 = (by.a_id == 0).nonempty(-1).ungroup_()
a1 = (by.a_id == 1).nonempty(-1).ungroup_()

by.pshape().qj('by.ps')
a0.pshape().qj('a0.ps')

sns.set_palette(sns.color_palette('Oranges_r', 18))

a1.stats.test.policy_mutual_information_through_time_self.np().mean(axis=0).apply(
    plot_with_baseline,
    a1.stats.test.policy_entropy_self.np().mean().qj('ent'),
    label=('normal agent, $\\beta$=' + a1.future_prediction_losses_scale_factor.preduce_eq_().pstr()).ungroup().preduce_eq(),
    title='',
    remaining=1,
    mean_std=False,
    c=plt.rcParams['axes.color_cycle'][:12],
    alpha=0.8,
    pepth=0)


sns.set_palette(sns.color_palette('Blues_r', 18))

a0.stats.test.policy_mutual_information_through_time_self.np().mean(axis=0).apply(
    plot_with_baseline,
    a0.stats.test.policy_entropy_self.np().mean().qj('ent'),
    label=('predictive agent, $\\beta$=' + a0.future_prediction_losses_scale_factor.preduce_eq_().pstr()).ungroup().preduce_eq(),
#     baseline_label=('agent ' + a0.a_id.preduce_eq_().pstr() + ', beta=' + a0.future_prediction_losses_scale_factor.preduce_eq_().pstr() + ' entropy').ungroup().preduce_eq(),
    title='',
    remaining=1,
    mean_std=False,
    c=plt.rcParams['axes.color_cycle'][:12],
    alpha=0.9,
    pepth=0)


# a1.stats.test.policy_mutual_information_through_time_self.apply(
#     plot_with_baseline,
#     a1.stats.test.policy_entropy_self.np().mean().qj('ent'),
#     label=('agent ' + a1.a_id.preduce_eq_().pstr() + ', beta=' + a1.future_prediction_losses_scale_factor.preduce_eq_().pstr() + ' mut. inf. through time').ungroup().preduce_eq(),
#     title='beta',
#     remaining=1,
#     c=plt.rcParams['axes.color_cycle'][:12],
#     pepth=0)

filename = os.path.join(CHARTS_DIR, 'ww_beta_multi_mut_inf_by_beta_by_a_id.pdf')
finalize_plot(filename=filename, figsize=(8, 6), xlabel='$t+k$', ylabel='nats of mutual information', legend_args=dict(ncol=2, loc='best'))
None

"""### ww_multi_sfpg{0|1} (predictive vs predictive or stop-gradient vs stop-gradient)

This chart compares Agents 0 and 1 trained above in ww_beta_multi with $\beta=0.5$ to three agent tournaments where all three agents are either predictive or non-predictive.

The blue lines are for the Agent 0s from above.  The green lines are the Agent 1s from above.  The red lines are the new predictive agents.  The yellow lines are the new non-predictive agents.

The conclusion here is that the improvement to the non-predictive agents is entirely due to the optimizer applying more of the loss to the predictive superstructure.  However, the inverse of the first prediction above appears to also be true: training predictive agents in an environment with non-predictive agents makes the predictive agents slightly harder to predict (also, the entropy goes down for those predictive agents, when makes the lower mutual information even more surprising, since lower entropy should make the classifiers' jobs easier).
"""

qj(len(agents), 'agents')
sagents = ((agents.exp_name == 'ww_multi_sfpg0') | (agents.exp_name == 'ww_multi_sfpg1'))
qj(len(sagents), 'sagents')

# sagents = (sagents.winner == True)
# qj(len(sagents), 'winners')

bagents = ((agents.exp_name == 'ww_beta_multi').future_prediction_losses_scale_factor == 0.5)
bagents = (bagents.a_id <= 1)
qj(len(bagents), 'bagents a_id=0,1 beta=0.5')
bagents_by = log_wins(bagents, 'future_prediction_losses_scale_factor', reverse=True)

by = log_wins(sagents, 'stop_future_prediction_gradients', 'stop_future_prediction_gradients')

sns.set_palette(sns.color_palette('Set2', 6))

# by.stats.test.policy_mutual_information_through_time_self.apply(
#     plot_with_baseline,
#     by.stats.test.policy_entropy_self.np_().mean(),
#     x_label='sfpg ' + by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pstr() + ' mut. inf. through time',
#     baseline_label='sfpg ' + by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pstr() + ' entropy',
#     title='predict v predict v predict or stop v stop v stop',
#     remaining=by.pleft(),
#     mean_std=False,
#     c=plt.rcParams['axes.color_cycle'][:2],
#     alpha=0.7,
#     pepth=2)


bagents_by.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    bagents_by.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + bagents_by.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + bagents_by.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='beta=' + bagents_by.ungroup(-1).future_prediction_losses_scale_factor.preduce_eq().pstr(),
    remaining=1,
    c=pist(plt.rcParams['axes.color_cycle'])[[0,1]],
    pepth=1)


by.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    by.stats.test.policy_entropy_self.np_().mean(),
    label='sfpg ' + by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='sfpg ' + by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pstr() + ' entropy',
    title='beta=' + by.ungroup(-1).future_prediction_losses_scale_factor.preduce_eq().pstr(),
    remaining=by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pleft().qj('pleft (should be [[1], [0]])', b=0),
    c=pist(plt.rcParams['axes.color_cycle'])[[3,5]],
    pepth=1)

# by.join().wins.apply(
#     plot_hist,
#     label='sfpg ' + by.ungroup_().stop_future_prediction_gradients.preduce_eq_().pstr() + ' wins',
#     title='predict v predict v predict or stop v stop v stop',
#     alpha=0.5, range=(30, 80),
#     pepth=0)

by.ungroup(-1).remix('a_id', 'name', 'exp_name', 'stop_future_prediction_gradients', 'winner', 'wins').stop_future_prediction_gradients.sortby().root().pd(index='name').head(1000)
None

"""### ww_gs_pred

These experiments do a grid search over $k$ at the optimal values found before (but assuming that $\beta=1/k$ is optimal, which we only have weak evidence for).  Here we look at $k\in[5,10,15,20,25]$.  We also vary the dropoff function, comparing inverse exponential to a quadratic dropoff.

The charts are the same three as we have seen above.

The conclusions here are:
 - It's easier to maintain high mutual information when making fewer predictions.  In other words, the predictability of 5 steps in the future is higher for agents that only predict 5 steps in the future than it is for agents that predict 10 steps in the future.  I suspect this effect would go away if the $\kappa$ parameters weren't constrained to be a convex combination, but I think that would be much harder to optimize.\
 - Predictive agents are still always more predictable than non-predictive agents when using the inverse exponential dropoff.
 - The quadratic dropoff function consistently performs worse for earlier predictions than the inverse exponential function, but predictability decays very slowly after that, so that at timestep $k$ it is always better than inverse exponential.
 - $1/k$ is almost certainly not the optimal value for $\beta$, as when $k$ is large, predictability of the predictive agent is indistinguishable from the non-predictive, and performance between the three agents is at chance.
"""

qj(len(agents), 'agents')
pagents = (agents.exp_name == 'ww_gs_pred')
qj(len(pagents), 'pagents')

pagents = ((pagents.winner == True) | ((pagents.runner_up == True) & (pagents.wins > 33.3333)))
qj(len(pagents), 'first place and strong second place')

# pagents = (pagents.winner == True)
# qj(len(pagents), 'winners')

log_wins(pagents, ())

log_wins(pagents, 'num_future_action_predictions')
log_wins(pagents, 'future_prediction_losses_dropoff')

by = log_wins(pagents, ('num_future_action_predictions', 'future_prediction_losses_dropoff'))

winners = (by.winner == True)
runners = (by.runner_up == True)

sns.set_palette(sns.color_palette('Set2', 3))

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='hparams:' + winners.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr().qj('hparams'),
    remaining=winners.pleft(),
    mean_std=False,
    c=plt.rcParams['axes.color_cycle'][:3],
    alpha=0.7,
    pepth=2)

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='hparams:' + winners.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr().qj('hparams'),
    remaining=winners.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=1),
    c=plt.rcParams['axes.color_cycle'][:3],
    pepth=1)

winners.wins.join().apply(
    plot_hist,
    label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' wins',
    title='hparams:' + by.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr(),
    secondary_x=runners.wins,
    secondary_label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' 2nd places',
    alpha=0.5, range=(30, 80),
    pepth=1)


by.ungroup(-1).remix('a_id', 'num_future_action_predictions', 'future_prediction_losses_dropoff', 'exp_name', 'winner', 'runner_up', 'wins').a_id.sortby().root().pd().head(1000)
# None

"""### ww_zs

These experiments are for a zero-sum variant of WaterWorld, where ever time an agent receives a reward $r$, the other $n-1$ agents receive $-r/(n-1)$, so the sum of the agents' scores is always 0.  I was hoping this would make it more explicit to agents that the environment is competitive, but I think the partially observed environment results in agents basically experiencing rewards as stochastic, because their observed behavior is not very good, and there's no performance difference between the agents, and very little difference in predictability.

It's possible that a different approach, where co-rewards are distributed by some function of distance, so that there's a higher probability of the source reward being observed, would work better, but after looking at these results, I decided not to pursue it further.
"""

qj(len(agents), 'agents')
zagents = (agents.exp_name == 'ww_gs_pred')
qj(len(zagents), 'zagents')

zagents = ((zagents.winner == True) | ((zagents.runner_up == True) & (zagents.wins > 33.3333)))
qj(len(zagents), 'first place and strong second place')

# zagents = (zagents.winner == True)
# qj(len(zagents), 'winners')

log_wins(zagents, ())

log_wins(zagents, 'num_future_action_predictions')
log_wins(zagents, 'future_prediction_losses_dropoff')

by = log_wins(zagents, ('num_future_action_predictions', 'future_prediction_losses_dropoff'))

winners = (by.winner == True)
runners = (by.runner_up == True)

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='hparams:' + winners.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr().qj('hparams'),
    remaining=winners.pleft(),
    mean_std=False,
    c=plt.rcParams['axes.color_cycle'][:3],
    alpha=0.7,
    pepth=2)

winners.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    winners.stats.test.policy_entropy_self.np_().mean(),
    label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' mut. inf. through time',
    baseline_label='agent ' + winners.ungroup_().a_id.preduce_eq_().pstr() + ' entropy',
    title='hparams:' + winners.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr().qj('hparams'),
    remaining=winners.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=1),
    c=plt.rcParams['axes.color_cycle'][:3],
    pepth=1)

winners.wins.join().apply(
    plot_hist,
    label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' wins',
    title='hparams:' + by.ungroup(-1)[('num_future_action_predictions', 'future_prediction_losses_dropoff')].preduce_eq().pstr(),
    secondary_x=runners.wins,
    secondary_label='agent ' + by.ungroup_().a_id.preduce_eq_().pstr() + ' 2nd places',
    alpha=0.5, range=(30, 80),
    pepth=1)


by.ungroup(-1).remix('a_id', 'num_future_action_predictions', 'future_prediction_losses_dropoff', 'exp_name', 'winner', 'wins').a_id.sortby().root().pd().head(1000)
# None

"""# ww_act_one"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_act_one/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 2).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

None

# import shutil
# nagents_dir = os.path.join(AGENTS_DIR, '..', 'non_predictive_agents')
# nagents = (agents.num_future_action_predictions == 1)
# nagents.plen().qj('nagents')
# nagents.path.apply(shutil.copy2, nagents_dir)
# None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions == 26)
bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.01)
bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

sns.set_palette(sns.color_palette('Set2', 5))

label = by[fields[-1]].preduce_eq__().ungroup_().pstr()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)
  
by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label=(label + ' mut. inf. through time').qj('label'),
    baseline_label=label + ' entropy',
    title=('beta ' + by[fields[0]].ungroup_().preduce_eq_().pstr()).qj('hparams', b=1),
    remaining=by[fields[0]].preduce_eq__().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:5],
    pepth=1)

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions > 1)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.01)
bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('num_future_action_predictions', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff', 'policy_layer_widths')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)
  
label = by[fields[-1]].preduce_eq__().ungroup_().pstr()

by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' mut. inf. through time',
    baseline_label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' entropy',
    title='hparams:' + by[fields[:-1]].preduce_eq__().ungroup_().qj('fields', b=0).pstr().qj('hparams', b=0),
    remaining=by.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:2],
    pepth=1)

None

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_one')

bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.01)
bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix('num_future_action_predictions', reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields)[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx.reward.np_().qj('rew', b=0).apply(
    plot,
    x=rmx.num_future_action_predictions.np_().mean().qj('nfap', b=0) - 1,
    label=rmx.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='solo play rewards (plw,vlw,beta)=' + rmx.ungroup(-1)[fields].preduce_eq().root()[fields[:-1]].pstr().qj('params', b=0),
    remaining=rmx.ungroup(-1)[fields].preduce_eq().pleft().qj('pleft', b=0),
    min_max=True,
    axis=-1,
    alpha=0.75,
    ylim=(0.0, 7.0),
    pepth=0)

None

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.01)
bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.2)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')
fields_plus = tuple(['mid', 'num_future_action_predictions'] + list(fields))

by = bfagents[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix(reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields_plus).num_future_action_predictions.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.reward.apply(qj, rmx.num_future_action_predictions.ungroup(-1).preduce_eq(), n=10)

params = (rmx.reward == rmx.reward.np().max())[fields_plus].qj('max-yielding params').pshape().qj('maxes ps').root()

sns.set_palette(sns.color_palette('Set2', 8)[1:])

rmx_x = rmx.num_future_action_predictions.ungroup(-1).preduce_eq() - 1

(rmx.reward
 .join().apply(
     plot,
     rmx_x,
     label='all',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.ungroup(-1)[fields_plus[1:]].sortby().root().num_future_action_predictions.groupby()
 .reward.np_().max().sortby_(reverse=True).uproot()
 .np().mean().qj('means').join().apply(
     lambda *_, **__: None,  # plot
     rmx_x,
     label='means',
     mean_std=False,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 1000).qj('reward top 1000')
 .qj('top 1000').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 1000',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 100).qj('reward top 100')
 .qj('top 100').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 100',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 10).qj('reward top 10')
 .qj('top 10').join().apply(
     plot,
     rmx_x,
     label='top 10',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.reward.np().max()
 .join().apply(
     plot,
     rmx_x,
     label='max score',
     mean_std=False,
     xlabel='$k$',
     ylabel='score',
     figsize=(6, 4),
     filename=os.path.join(CHARTS_DIR, 'ww_act_one_reward.pdf'),
     legend_args=dict(loc='top', ncol=5),
     ylim=(4.5, 7.0),
 )
)

((rmx.num_future_action_predictions == 1).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=1')
((rmx.num_future_action_predictions == 6).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=6')
((rmx.num_future_action_predictions == 11).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=11')
((rmx.num_future_action_predictions == 16).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=16')
((rmx.num_future_action_predictions == 21).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=21')
((rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=26')

(rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().pd().head(100)

"""# ww_act_two"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_act_two/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 2).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

def plot_counts(agents, field, index='a_id', x='num_future_action_predictions', reverse=False, title='future_prediction_losses_scale_factor', baseline=0.5):
  by = agents[field].sortby(reverse=reverse).groupby()[index].sortby_().groupby()[x].sortby__().groupby()
  by_labels = ('agent ' + by[index].preduce_eq___().pstr() + ' ' + by[field].preduce_eq___().pstr()).ungroup__(-1).preduce_eq__().ungroup_()
  by_titles = by[title].ungroup__().preduce_eq__().ungroup(-1).pstr()
  by_x = by[x].preduce_eq___().ungroup__() - 1
  by.plen___().ungroup__().qj('plen', b=0).apply(
      lambda y, x, *a, **kw: plot_with_baseline(qj(y, 'y', b=0) / 25.0, baseline, qj(x, 'x', b=0), *a, **kw),
      x=by_x,
      label=by_labels,
      title=by_titles,
      remaining=by_titles.pleft(),
      mean_std=False,
      ylim=(0.0, 1.0),
      pepth=1,
  )
  return by


qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bagents = (agents.exp_name == 'ww_act_two')
qj(len(bagents), 'bagents')

# bagents = ((bagents.num_future_action_predictions == 6))
bagents = ((bagents.policy_layer_widths == '100_40'))
# bagents = ((bagents.future_prediction_losses_dropoff == '1m_exp'))

bagents = (bagents.future_prediction_losses_scale_factor >= 0.01)
bagents = (bagents.future_prediction_losses_scale_factor <= 0.5)

bagents = ((bagents.a_id == 0))
qj(len(bagents), 'bagents')

bagents = ((bagents.winner == True) & (bagents.wins >= 50))

qj(len(bagents), 'winners')

fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')


# log_wins(bagents, ())

# log_wins(bagents, 'policy_layer_widths')
# log_wins(bagents, 'value_layer_width')
# log_wins(bagents, 'future_prediction_losses_scale_factor')
# log_wins(bagents, 'future_prediction_losses_dropoff')
# by = log_wins(bagents, 'num_future_action_predictions')

# log_wins(bagents, ('policy_layer_widths', 'value_layer_width'))
# log_wins(bagents, ('policy_layer_widths', 'value_layer_width', 'num_future_action_predictions'))
# log_wins(bagents, ('future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff'))
# log_wins(bagents, ('future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff', 'num_future_action_predictions'))

# by = log_wins(bagents, fields)

# by = plot_counts(bagents, fields)

sns.set_palette(sns.color_palette('Set2', 13)[1:])

fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_dropoff', 'future_prediction_losses_scale_factor')
by = plot_counts(bagents, fields, title='future_prediction_losses_dropoff')

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bagents = (agents.exp_name == 'ww_act_two')
qj(len(bagents), 'bagents')

# bagents = ((bagents.num_future_action_predictions == 6))
bagents = ((bagents.policy_layer_widths == '40_20'))
# bagents = ((bagents.future_prediction_losses_dropoff == '1m_exp'))

bagents = (bagents.future_prediction_losses_scale_factor >= 0.01)
bagents = (bagents.future_prediction_losses_scale_factor <= 0.09)

# bagents = ((bagents.a_id == 0))

bagents = ((bagents.winner == True) & (bagents.wins >= 50))

qj(len(bagents), 'winners')

fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_dropoff', 'future_prediction_losses_scale_factor', 'num_future_action_predictions')


# log_wins(bagents, ())

# log_wins(bagents, 'policy_layer_widths')
# log_wins(bagents, 'value_layer_width')
# log_wins(bagents, 'future_prediction_losses_scale_factor')
# log_wins(bagents, 'future_prediction_losses_dropoff')
# by = log_wins(bagents, 'num_future_action_predictions')

# log_wins(bagents, ('policy_layer_widths', 'value_layer_width'))
# log_wins(bagents, ('policy_layer_widths', 'value_layer_width', 'num_future_action_predictions'))
# log_wins(bagents, ('future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff'))
# log_wins(bagents, ('future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff', 'num_future_action_predictions'))

by = log_wins(bagents, fields)

sns.set_palette(sns.color_palette('Set2', 13)[1:])

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  qj(x)
  qj(ent)
  qj(lsf)
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)

dropoff_label = by.future_prediction_losses_dropoff.preduce_eq__().ungroup_().pstr()
label = (dropoff_label + ' agent ' + by.ungroup(-1).a_id.preduce_eq().pstr()).qj('label', b=0)
title = ('hparams:' + by[fields[:-1]].preduce_eq__().ungroup_().pstr()).qj('title', b=0)

by.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
#     lambda x, baseline, title='', *a, **kw: plot_with_baseline(qj(x, l=lambda x: (len(x), [np.array(e).shape for e in x])), baseline, title=qj(title, l=lambda x: len(x)), *a, **kw),
    by.stats.test.policy_entropy_self.np_().mean().qj('ent'),
#     lsf=by.loss_scale_factors,
#     lab=dropoff_label,
    label=label + ' mut. inf. through time',
    baseline_label=label + ' entropy',
    title=title,
    remaining=title.pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
    c=plt.rcParams['axes.color_cycle'][:2],
    ylim=(0.0, 1.6),
    pepth=1)

bagents.ungroup(-1)[fields].sortby().root().remix('a_id', 'mid', 'exp_name', 'winner', 'wins', 'stop_future_prediction_gradients', *fields).a_id.sortby().root().pd(index='mid').head(1000)
# None

(agents.mid == 'mid_7b97b40f71451b8644a96528e12367a1-li-d').remix('mid', 'a_id', 'wins', *fields).pd()

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_two')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.a_id == 0)
bfagents = (bfagents.policy_layer_widths == '40_20')
bfagents = (bfagents.value_layer_width == 256)
bfagents = (bfagents.future_prediction_losses_dropoff == 'const')
bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.06)
bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.06)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')
fields_plus = tuple(['mid', 'num_future_action_predictions'] + list(fields))

by = bfagents[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix('wins', *fields_plus).num_future_action_predictions.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.wins.apply(qj, rmx.num_future_action_predictions.ungroup(-1).preduce_eq(), n=10)

params = (rmx.wins == rmx.wins.np().max())[fields_plus].qj('max-yielding params').root()

sns.set_palette(sns.color_palette('Set2', 8)[1:])

rmx_x = rmx.num_future_action_predictions.ungroup(-1).preduce_eq() - 1

(rmx.wins
 .join().apply(
     plot,
     rmx_x,
     label='all',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.ungroup(-1)[fields_plus[1:]].sortby().root().num_future_action_predictions.groupby()
 .wins.np_().max().sortby_(reverse=True).uproot()
 .np().mean().qj('means').join().apply(
     lambda *_, **__: None,  # plot
     rmx_x,
     label='means',
     mean_std=False,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 1000)
 .join().apply(
     plot,
     rmx_x,
     label='top 1000',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 100)
 .join().apply(
     plot,
     rmx_x,
     label='top 100',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 10).qj('wins top 10')
 .join().apply(
     plot,
     rmx_x,
     label='top 10',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.wins.np().max()
 .join().apply(
     plot,
     rmx_x,
     label='max win percent',
     mean_std=False,
     xlabel='$k$',
     ylabel='win percent',
     figsize=(6, 4),
     filename=os.path.join(CHARTS_DIR, 'ww_act_two_wins.pdf'),
     legend_args=dict(loc='top', ncol=5),
#      ylim=(4.5, 7.0),
 )
)

((rmx.num_future_action_predictions == 6).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=6'))
((rmx.num_future_action_predictions == 11).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=11'))
((rmx.num_future_action_predictions == 16).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=16'))
((rmx.num_future_action_predictions == 21).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=21'))
((rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=26'))

(rmx.num_future_action_predictions <= 26).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().pd().head(1000)

"""### mpl samples"""

from matplotlib.path import Path
from matplotlib.spines import Spine
from matplotlib.projections.polar import PolarAxes
from matplotlib.projections import register_projection


def radar_factory(num_vars, frame='circle'):
    """Create a radar chart with `num_vars` axes.

    This function creates a RadarAxes projection and registers it.

    Parameters
    ----------
    num_vars : int
        Number of variables for radar chart.
    frame : {'circle' | 'polygon'}
        Shape of frame surrounding axes.

    """
    # calculate evenly-spaced axis angles
    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)
    # rotate theta such that the first axis is at the top
    theta += np.pi/2

    def draw_poly_patch(self):
        verts = unit_poly_verts(theta)
        return plt.Polygon(verts, closed=True, edgecolor='k')

    def draw_circle_patch(self):
        # unit circle centered on (0.5, 0.5)
        return plt.Circle((0.5, 0.5), 0.5)

    patch_dict = {'polygon': draw_poly_patch, 'circle': draw_circle_patch}
    if frame not in patch_dict:
        raise ValueError('unknown value for `frame`: %s' % frame)

    class RadarAxes(PolarAxes):

        name = 'radar'
        # use 1 line segment to connect specified points
        RESOLUTION = 1
        # define draw_frame method
        draw_patch = patch_dict[frame]

        def fill(self, *args, **kwargs):
            """Override fill so that line is closed by default"""
            closed = kwargs.pop('closed', True)
            return super(RadarAxes, self).fill(closed=closed, *args, **kwargs)

        def plot(self, *args, **kwargs):
            """Override plot so that line is closed by default"""
            lines = super(RadarAxes, self).plot(*args, **kwargs)
            for line in lines:
                self._close_line(line)

        def _close_line(self, line):
            x, y = line.get_data()
            # FIXME: markers at x[0], y[0] get doubled-up
            if x[0] != x[-1]:
                x = np.concatenate((x, [x[0]]))
                y = np.concatenate((y, [y[0]]))
                line.set_data(x, y)

        def set_varlabels(self, labels):
            self.set_thetagrids(np.degrees(theta), labels)

        def _gen_axes_patch(self):
            return self.draw_patch()

        def _gen_axes_spines(self):
            if frame == 'circle':
                return PolarAxes._gen_axes_spines(self)
            # The following is a hack to get the spines (i.e. the axes frame)
            # to draw correctly for a polygon frame.

            # spine_type must be 'left', 'right', 'top', 'bottom', or `circle`.
            spine_type = 'circle'
            verts = unit_poly_verts(theta)
            # close off polygon by repeating first vertex
            verts.append(verts[0])
            path = Path(verts)

            spine = Spine(self, spine_type, path)
            spine.set_transform(self.transAxes)
            return {'polar': spine}

    register_projection(RadarAxes)
    return theta


def unit_poly_verts(theta):
    """Return vertices of polygon for subplot axes.

    This polygon is circumscribed by a unit circle centered at (0.5, 0.5)
    """
    x0, y0, r = [0.5] * 3
    verts = [(r*np.cos(t) + x0, r*np.sin(t) + y0) for t in theta]
    return verts


def example_data():
    # The following data is from the Denver Aerosol Sources and Health study.
    # See  doi:10.1016/j.atmosenv.2008.12.017
    #
    # The data are pollution source profile estimates for five modeled
    # pollution sources (e.g., cars, wood-burning, etc) that emit 7-9 chemical
    # species. The radar charts are experimented with here to see if we can
    # nicely visualize how the modeled source profiles change across four
    # scenarios:
    #  1) No gas-phase species present, just seven particulate counts on
    #     Sulfate
    #     Nitrate
    #     Elemental Carbon (EC)
    #     Organic Carbon fraction 1 (OC)
    #     Organic Carbon fraction 2 (OC2)
    #     Organic Carbon fraction 3 (OC3)
    #     Pyrolized Organic Carbon (OP)
    #  2)Inclusion of gas-phase specie carbon monoxide (CO)
    #  3)Inclusion of gas-phase specie ozone (O3).
    #  4)Inclusion of both gas-phase species is present...
    data = [
        ['Sulfate', 'Nitrate', 'EC', 'OC1', 'OC2', 'OC3', 'OP', 'CO', 'O3'],
        ('Basecase', [
            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],
            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],
            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],
            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],
            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]]),
        ('With CO', [
            [0.88, 0.02, 0.02, 0.02, 0.00, 0.05, 0.00, 0.05, 0.00],
            [0.08, 0.94, 0.04, 0.02, 0.00, 0.01, 0.12, 0.04, 0.00],
            [0.01, 0.01, 0.79, 0.10, 0.00, 0.05, 0.00, 0.31, 0.00],
            [0.00, 0.02, 0.03, 0.38, 0.31, 0.31, 0.00, 0.59, 0.00],
            [0.02, 0.02, 0.11, 0.47, 0.69, 0.58, 0.88, 0.00, 0.00]]),
        ('With O3', [
            [0.89, 0.01, 0.07, 0.00, 0.00, 0.05, 0.00, 0.00, 0.03],
            [0.07, 0.95, 0.05, 0.04, 0.00, 0.02, 0.12, 0.00, 0.00],
            [0.01, 0.02, 0.86, 0.27, 0.16, 0.19, 0.00, 0.00, 0.00],
            [0.01, 0.03, 0.00, 0.32, 0.29, 0.27, 0.00, 0.00, 0.95],
            [0.02, 0.00, 0.03, 0.37, 0.56, 0.47, 0.87, 0.00, 0.00]]),
        ('CO & O3', [
            [0.87, 0.01, 0.08, 0.00, 0.00, 0.04, 0.00, 0.00, 0.01],
            [0.09, 0.95, 0.02, 0.03, 0.00, 0.01, 0.13, 0.06, 0.00],
            [0.01, 0.02, 0.71, 0.24, 0.13, 0.16, 0.00, 0.50, 0.00],
            [0.01, 0.03, 0.00, 0.28, 0.24, 0.23, 0.00, 0.44, 0.88],
            [0.02, 0.00, 0.18, 0.45, 0.64, 0.55, 0.86, 0.00, 0.16]])
    ]
    return data


N = 9
theta = radar_factory(N, frame='polygon')

data = example_data()
spoke_labels = data.pop(0)

fig, axes = plt.subplots(figsize=(9, 9), nrows=2, ncols=2,
                         subplot_kw=dict(projection='radar'))
fig.subplots_adjust(wspace=0.25, hspace=0.20, top=0.85, bottom=0.05)

colors = ['b', 'r', 'g', 'm', 'y']
# Plot the four cases from the example data on separate axes
for ax, (title, case_data) in zip(axes.flatten(), data):
    ax.set_rgrids([0.2, 0.4, 0.6, 0.8])
    ax.set_title(title, weight='bold', size='medium', position=(0.5, 1.1),
                 horizontalalignment='center', verticalalignment='center')
    for d, color in zip(case_data, colors):
        ax.plot(theta, d, color=color)
        ax.fill(theta, d, facecolor=color, alpha=0.25)
    ax.set_varlabels(spoke_labels)

# add legend relative to top-left plot
ax = axes[0, 0]
labels = ('Factor 1', 'Factor 2', 'Factor 3', 'Factor 4', 'Factor 5')
legend = ax.legend(labels, loc=(0.9, .95),
                   labelspacing=0.1, fontsize='small')

fig.text(0.5, 0.965, '5-Factor Solution Profiles Across Four Scenarios',
         horizontalalignment='center', color='black', weight='bold',
         size='large')

plt.show()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))

# generate some random test data
all_data = [np.random.normal(0, std, 100) for std in range(6, 10)]

# plot violin plot
axes[0].violinplot(all_data,
                   showmeans=False,
                   showmedians=True)
axes[0].set_title('violin plot')

# plot box plot
axes[1].boxplot(all_data)
axes[1].set_title('box plot')

# adding horizontal grid lines
for ax in axes:
    ax.yaxis.grid(True)
    ax.set_xticks([y+1 for y in range(len(all_data))])
    ax.set_xlabel('xlabel')
    ax.set_ylabel('ylabel')

# add x-tick labels
plt.setp(axes, xticks=[y+1 for y in range(len(all_data))],
         xticklabels=['x1', 'x2', 'x3', 'x4'])
plt.show()

x = np.random.randn(1000)
y = np.random.randn(1000) + 5

# normal distribution center at x=0 and y=5
plt.hist2d(x, y, bins=40)
plt.show()

"""# ww_rew_one"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_rew_one/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 2).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_rew_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions == 26)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

# bfagents = (bfagents.policy_predict_rewards_weight >= 0.01)
# bfagents = (bfagents.policy_predict_rewards_weight <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('policy_predict_rewards_weight', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

sns.set_palette(sns.color_palette('Set2', 5))

label = by[fields[-1]].preduce_eq__().ungroup_().pstr()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)
  
by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label=(label + ' mut. inf. through time').qj('label'),
    baseline_label=label + ' entropy',
    title=('beta ' + by[fields[0]].ungroup_().preduce_eq_().pstr()).qj('hparams', b=1),
    remaining=by[fields[0]].preduce_eq__().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:5],
    pepth=1)

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_rew_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions > 1)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

# bfagents = (bfagents.policy_predict_rewards_weight >= 0.01)
# bfagents = (bfagents.policy_predict_rewards_weight <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('num_future_action_predictions', 'policy_predict_rewards_weight', 'future_prediction_losses_dropoff', 'policy_layer_widths')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)
  
label = by[fields[-1]].preduce_eq__().ungroup_().pstr()

by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' mut. inf. through time',
    baseline_label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' entropy',
    title='hparams:' + by[fields[:-1]].preduce_eq__().ungroup_().qj('fields', b=0).pstr().qj('hparams', b=0),
    remaining=by.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:2],
    pepth=1)

None

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_rew_one')

# bfagents = (bfagents.policy_predict_rewards_weight >= 0.01)
# bfagents = (bfagents.policy_predict_rewards_weight <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('policy_layer_widths', 'value_layer_width', 'policy_predict_rewards_weight', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix('num_future_action_predictions', reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields)[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx.reward.np_().qj('rew', b=0).apply(
    plot,
    x=rmx.num_future_action_predictions.np_().mean().qj('nfap', b=0) - 1,
    label=rmx.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='solo play rewards (plw,vlw,beta)=' + rmx.ungroup(-1)[fields].preduce_eq().root()[fields[:-1]].pstr().qj('params', b=0),
    remaining=rmx.ungroup(-1)[fields].preduce_eq().pleft().qj('pleft', b=0),
    min_max=True,
    axis=-1,
    alpha=0.75,
    ylim=(0.0, 7.0),
    pepth=0)

None

qj(len(agents), 'agents')
bfagents = ((agents.exp_name == 'ww_rew_one') | (agents.num_future_action_predictions == 1))
qj(len(bfagents), 'bfagents')

bfagents = (
    ((bfagents.policy_predict_rewards_weight >= 0.01) & (bfagents.policy_predict_rewards_weight <= 0.2))
    | ((bfagents.num_future_action_predictions == 1) & (bfagents.future_prediction_losses_scale_factor >= 0.01) & (bfagents.future_prediction_losses_scale_factor <= 0.2))
)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'policy_predict_rewards_weight', 'future_prediction_losses_dropoff', 'future_prediction_losses_scale_factor')
fields_plus = tuple(['mid', 'num_future_action_predictions'] + list(fields))

by = bfagents[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix(reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields_plus).num_future_action_predictions.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.reward.apply(qj, rmx.num_future_action_predictions.ungroup(-1).preduce_eq(), n=10)

params = (rmx.reward == rmx.reward.np().max())[fields_plus].qj('max-yielding params').pshape().qj('maxes ps').root()
# (rmx.ungroup(-1)[fields_plus[1:]] == params.ungroup(-1)[fields_plus[1:]]).nonempty(-1).pshape().qj('ps').root().reward.qj('reward').sortby(reverse=True).qj('reward.sorted')
# .root()[fields_plus[1:]].sortby().groupby().pshape().qj('fields ps').root().reward.np().qj('reward [:-5]').mean().qj('means').root().ungroup(-1)[fields_plus[1:]].preduce_eq().qj('fields')

sns.set_palette(sns.color_palette('Set2', 8)[1:])

rmx_x = rmx.num_future_action_predictions.ungroup(-1).preduce_eq() - 1

(rmx.reward
 .join().apply(
     plot,
     rmx_x,
     label='all',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.ungroup(-1)[fields_plus[1:]].sortby().root().num_future_action_predictions.groupby()
 .reward.np_().max().sortby_(reverse=True).uproot()
 .np().mean().qj('means').join().apply(
     lambda *_, **__: None,  # plot
     rmx_x,
     label='means',
     mean_std=False,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 1000).qj('reward top 1000')
 .qj('top 1000').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 1000',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 100).qj('reward top 100')
 .qj('top 100').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 100',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 10).qj('reward top 10')
 .qj('top 10').join().apply(
     plot,
     rmx_x,
     label='top 10',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.reward.np().max()
 .join().apply(
     plot,
     rmx_x,
     label='max score',
     mean_std=False,
     xlabel='$k$',
     ylabel='score',
     figsize=(6, 4),
     filename=os.path.join(CHARTS_DIR, 'ww_rew_one_reward.pdf'),
     legend_args=dict(loc='top', ncol=5),
     ylim=(4.5, 7.0),
 )
)

((rmx.num_future_action_predictions == 1).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=1')
((rmx.num_future_action_predictions == 6).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_rewards_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=6')
((rmx.num_future_action_predictions == 11).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_rewards_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=11')
((rmx.num_future_action_predictions == 16).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_rewards_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=16')
((rmx.num_future_action_predictions == 21).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_rewards_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=21')
((rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_rewards_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=26')

(rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().pd().head(100)

"""# ww_obs_one"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_obs_one/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 2).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_obs_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions == 26)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
bfagents = ((bfagents.future_prediction_losses_dropoff == '1m_exp'))

# bfagents = (bfagents.policy_predict_observations_weight >= 0.01)
bfagents = (bfagents.policy_predict_observations_weight == 10.0)

qj(len(bfagents), 'bfagents')

fields = ('policy_predict_observations_weight', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

sns.set_palette(sns.color_palette('Set2', 5)[1:])

label = '$\\beta=10$'

by.stats.test.policy_mutual_information_through_time_self.apply(
    plot_with_baseline,
    by.stats.test.policy_entropy_self.np_().mean(),
    label=label + ' mutual information through time',
    baseline_label=label + ' entropy',
    remaining=0,
    figsize=(6, 4),
    xlabel='$k$',
    ylabel='information (nats)',
    filename=os.path.join(CHARTS_DIR, 'ww-obs-one-fpld-1mexp-nfap26-ppow10-mut-inf.pdf'),
    legend_args=dict(loc='top', ncol=5),
    ylim=(0.0, 1.6),
    pepth=1)

None

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_obs_one')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions > 1)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

# bfagents = (bfagents.policy_predict_observations_weight >= 0.01)
# bfagents = (bfagents.policy_predict_observations_weight <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('num_future_action_predictions', 'policy_predict_observations_weight', 'future_prediction_losses_dropoff', 'policy_layer_widths')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)
  
label = by[fields[-1]].preduce_eq__().ungroup_().pstr()

by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' mut. inf. through time',
    baseline_label='plw ' + by[fields[-1]].preduce_eq__().ungroup_().pstr() + ' entropy',
    title='hparams:' + by[fields[:-1]].preduce_eq__().ungroup_().qj('fields', b=0).pstr().qj('hparams', b=0),
    remaining=by.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:2],
    pepth=1)

None

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_obs_one')

# bfagents = (bfagents.policy_predict_observations_weight >= 0.01)
# bfagents = (bfagents.policy_predict_observations_weight <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('policy_layer_widths', 'value_layer_width', 'policy_predict_observations_weight', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix('num_future_action_predictions', reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields)[fields].qj('fields', b=0).sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx.reward.np_().qj('rew', b=0).apply(
    plot,
    x=rmx.num_future_action_predictions.np_().mean().qj('nfap', b=0) - 1,
    label=rmx.ungroup(-1)[fields].preduce_eq().pstr() + ' reward',
    title='solo play rewards (plw,vlw,beta)=' + rmx.ungroup(-1)[fields].preduce_eq().root()[fields[:-1]].pstr().qj('params', b=0),
    remaining=rmx.ungroup(-1)[fields].preduce_eq().pleft().qj('pleft', b=0),
    min_max=True,
    axis=-1,
    alpha=0.75,
    ylim=(0.0, 7.0),
    pepth=0)

None

bfagents = ((agents.exp_name == 'ww_obs_one') | (agents.num_future_action_predictions == 1))
qj(len(bfagents), 'bfagents')

bfagents = (
    ((bfagents.policy_predict_observations_weight >= 0.01) & (bfagents.policy_predict_observations_weight <= 0.2))
    | ((bfagents.num_future_action_predictions == 1) & (bfagents.future_prediction_losses_scale_factor >= 0.01) & (bfagents.future_prediction_losses_scale_factor <= 0.2))
)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'policy_predict_observations_weight', 'future_prediction_losses_dropoff', 'future_prediction_losses_scale_factor')
fields_plus = tuple(['mid', 'num_future_action_predictions'] + list(fields))

by = bfagents[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix(reward=by.stats.test.total_reward.np__().mean().ungroup(-1), *fields_plus).num_future_action_predictions.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.reward.apply(qj, rmx.num_future_action_predictions.ungroup(-1).preduce_eq(), n=10)

params = (rmx.reward == rmx.reward.np().max())[fields_plus].qj('max-yielding params').pshape().qj('maxes ps').root()
# (rmx.ungroup(-1)[fields_plus[1:]] == params.ungroup(-1)[fields_plus[1:]]).nonempty(-1).pshape().qj('ps').root().reward.qj('reward').sortby(reverse=True).qj('reward.sorted')
# .root()[fields_plus[1:]].sortby().groupby().pshape().qj('fields ps').root().reward.np().qj('reward [:-5]').mean().qj('means').root().ungroup(-1)[fields_plus[1:]].preduce_eq().qj('fields')

sns.set_palette(sns.color_palette('Set2', 8)[1:])

rmx_x = rmx.num_future_action_predictions.ungroup(-1).preduce_eq() - 1

(rmx.reward
 .join().apply(
     plot,
     rmx_x,
     label='all',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.ungroup(-1)[fields_plus[1:]].sortby().root().num_future_action_predictions.groupby()
 .reward.np_().max().sortby_(reverse=True).uproot()
 .np().mean().qj('means').join().apply(
     lambda *_, **__: None,  # plot
     rmx_x,
     label='means',
     mean_std=False,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 1000).qj('reward top 1000')
 .qj('top 1000').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 1000',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 100).qj('reward top 100')
 .qj('top 100').join().apply(
     plot,
     rmx_x.qj('rmx_x'),
     label='top 100',
     mean_std=True,
     axis=-1,
     remaining=1,
 ).root()[0].root().apply_('___getslice__', 0, 10).qj('reward top 10')
 .qj('top 10').join().apply(
     plot,
     rmx_x,
     label='top 10',
     mean_std=True,
     axis=-1,
     remaining=1,
 )
)

(rmx.reward.np().max()
 .join().apply(
     plot,
     rmx_x,
     label='max score',
     mean_std=False,
     xlabel='$k$',
     ylabel='score',
     figsize=(6, 4),
     filename=os.path.join(CHARTS_DIR, 'ww_obs_one_reward.pdf'),
     legend_args=dict(loc='top', ncol=5),
     ylim=(4.5, 7.0),
 )
)

((rmx.num_future_action_predictions == 1).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).future_prediction_losses_scale_factor == 0.01).plen().qj('Top 100 with beta=0.01, nfap=1')
((rmx.num_future_action_predictions == 6).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_observations_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=6')
((rmx.num_future_action_predictions == 11).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_observations_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=11')
((rmx.num_future_action_predictions == 16).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_observations_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=16')
((rmx.num_future_action_predictions == 21).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_observations_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=21')
((rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).policy_predict_observations_weight == 0.01).plen().qj('Top 100 with beta=0.01, nfap=26')

(rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().reward.np().max().sortby(reverse=True).root().pd().head(100)

"""# ww_act_non"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_act_non/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 2).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_non')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.a_id == 0)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')
fields_plus = tuple(['mid', 'num_future_action_predictions'] + list(fields))

by = bfagents[fields].sortby().groupby().num_future_action_predictions.sortby_().groupby()

rmx = by.ungroup(-1).remix('wins', *fields_plus).num_future_action_predictions.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.wins.apply(qj, rmx.num_future_action_predictions.ungroup(-1).preduce_eq(), n=10)

params = (rmx.wins == rmx.wins.np().max())[fields_plus].qj('max-yielding params').root()

# sns.set_palette(sns.color_palette('Set2', 8)[1:])

# rmx_x = rmx.num_future_action_predictions.ungroup(-1).preduce_eq() - 1

# (rmx.wins
#  .join().apply(
#      plot,
#      rmx_x,
#      label='all',
#      mean_std=True,
#      axis=-1,
#      remaining=1,
#  )
# )

# (rmx.ungroup(-1)[fields_plus[1:]].sortby().root().num_future_action_predictions.groupby()
#  .wins.np_().max().sortby_(reverse=True).uproot()
#  .np().mean().qj('means').join().apply(
#      lambda *_, **__: None,  # plot
#      rmx_x,
#      label='means',
#      mean_std=False,
#      remaining=1,
#  ).root()[0].root().apply_('___getslice__', 0, 1000)
#  .join().apply(
#      plot,
#      rmx_x,
#      label='top 1000',
#      mean_std=True,
#      axis=-1,
#      remaining=1,
#  ).root()[0].root().apply_('___getslice__', 0, 100)
#  .join().apply(
#      plot,
#      rmx_x,
#      label='top 100',
#      mean_std=True,
#      axis=-1,
#      remaining=1,
#  ).root()[0].root().apply_('___getslice__', 0, 10).qj('wins top 10')
#  .join().apply(
#      plot,
#      rmx_x,
#      label='top 10',
#      mean_std=True,
#      axis=-1,
#      remaining=1,
#  )
# )

# (rmx.wins.np().max()
#  .join().apply(
#      plot,
#      rmx_x,
#      label='max win percent',
#      mean_std=False,
#      xlabel='$k$',
#      ylabel='win percent',
#      figsize=(6, 4),
#      filename=os.path.join(CHARTS_DIR, 'ww_act_two_wins.pdf'),
#      legend_args=dict(loc='top', ncol=5),
# #      ylim=(4.5, 7.0),
#  )
# )

# ((rmx.num_future_action_predictions == 6).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=6'))
# ((rmx.num_future_action_predictions == 11).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=11'))
# ((rmx.num_future_action_predictions == 16).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=16'))
# ((rmx.num_future_action_predictions == 21).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=21'))
# ((rmx.num_future_action_predictions == 26).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().apply('___getslice__', 0, 100).join().wins.np().mean().qj('Top 100 wins, nfap=26'))

(rmx.num_future_action_predictions <= 26).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().pd().head(1000)

"""# ww_act_obs_rew_non"""

AGENTS_DIR = '{BASE_SAVE_DIR}/ww_act_obs_rew_non/agents'.format(**locals())
cached_agents = {}
agent_files = gfile.Glob(os.path.join(AGENTS_DIR, '*minimal.p'))

qj('Building agents')
agents = (pist(get_agents(agent_files)) != None)
agents.plen().qj('Num agents')

agents.wins = 0
agents.winner = False
wins = (((agents.mid.groupby().apply(len) == 4).stats.test.wins.np().sum() - 100).apply(np.abs) <= 1e-1).stats.test.wins.np_().sum()
(wins == wins.np().max()).winner = True
wins.root().wins = wins

wins[:2].qj('First 2 wins').root().winner.qj('winners')

set_loss_scale_factors(agents)

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_obs_rew_non')
qj(len(bfagents), 'bfagents')

# bfagents = (bfagents.a_id == 0)

qj(len(bfagents), 'bfagents')


fields = ('policy_layer_widths', 'value_layer_width', 'future_prediction_losses_scale_factor', 'future_prediction_losses_dropoff')
fields_plus = tuple(['mid', 'a_id'] + list(fields))

by = bfagents[fields].sortby().groupby().a_id.sortby_().groupby()

rmx = by.ungroup(-1).remix('winner', 'wins', total_reward=by.ungroup(-1).stats.test.total_reward.np().mean(), *fields_plus).a_id.sortby().groupby()[fields_plus[1:]].sortby().root()

rmx.wins.apply(qj, 'wins ' + rmx.a_id.ungroup(-1).preduce_eq().pstr(), n=10)
rmx.total_reward.apply(qj, 'reward ' + rmx.a_id.ungroup(-1).preduce_eq().pstr(), n=10)

params = (rmx.wins == rmx.wins.np().max())[fields_plus].qj('max-yielding params').root()

(bfagents.mid == 'mid_3f007d252532072b961088e931de68e9-ok-d').recording_dirs.qj('dirs')

(rmx.winner == True).ungroup(-1)[fields_plus[1:]].sortby().root().wins.np().max().sortby(reverse=True).root().pd().head(1000)

qj.MAX_FRAME_LOGS = 1000

qj(len(agents), 'agents')
bfagents = (agents.exp_name == 'ww_act_obs_rew_non')
qj(len(bfagents), 'bfagents')

bfagents = (bfagents.num_future_action_predictions > 1)
# bfagents = ((bfagents.policy_layer_widths == '100_40'))
# bfagents = ((bfagents.future_prediction_losses_dropoff == 'inv_exp'))

# bfagents = (bfagents.future_prediction_losses_scale_factor >= 0.01)
# bfagents = (bfagents.future_prediction_losses_scale_factor <= 0.2)

qj(len(bfagents), 'bfagents')

fields = ('a_id', 'future_prediction_losses_dropoff')

by = bfagents[fields].qj('fields', b=0).sortby().groupby().a_id.sortby_().groupby()

def dual_plot(x, ent, lsf, lab, title='', *a, **kw):
  color = plot(lsf, ls=':', label=lab, title=title, remaining=1)
  kw.update(color=color)
  plot_with_baseline(x, ent, title=title, *a, **kw)

label = by[fields].preduce_eq__().ungroup_().pstr()

by.stats.test.policy_mutual_information_through_time_self.apply(
    dual_plot,
    ent=by.stats.test.policy_entropy_self.np_().mean(),
    lsf=by.loss_scale_factors,
    lab=label,
    label=label + ' mut. inf. through time',
    baseline_label=label + ' entropy',
    title='',
    remaining=by.ungroup_().a_id.preduce_eq_().pleft().qj('pleft (should be like [[35, 34, 33], ...])', b=0),
#     c=plt.rcParams['axes.color_cycle'][:2],
    pepth=1)

None